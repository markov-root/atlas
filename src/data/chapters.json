{
  "streams": [
    {
      "id": "fundamentals",
      "title": "Fundamental AI Safety",
      "description": "Core concepts every AI safety researcher should understand",
      "chapters": [
        {
          "id": "ch1",
          "number": 1,
          "title": "Capabilities",
          "subtitle": "Why might AIs keep growing both increasingly general and capable?",
          "description": "AI models are moving from specialized tools into increasingly general-purpose systems that can handle complex tasks. In this chapter we talk about empirical trends which show that scaling up - using more data, compute, and parameters - is leading to steady gains in both capabilities and generality.",
          "texture": "",
          "reading_time_core": "25 min",
          "reading_time_optional": "8 min",
          "reading_time_appendix": "12 min",
          "resources": {
            "chapter": "/chapters/01",
            "video": "https://www.youtube.com/watch?v=J_iMeH1hb9M",
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1L32xCVUCWEsm-x8UZ3GSTgKnmBcC7rJQLLIh9wGLj40/edit?usp=sharing"
          },
          "quote": {
            "text": "The continued advancement of AI capabilities represents both tremendous opportunity and significant challenge for humanity.",
            "author": "Stuart Russell"
          }
        },
        {
          "id": "ch2",
          "number": 2,
          "title": "Risks",
          "subtitle": "What can go wrong if AI keeps becoming more capable?",
          "description": "Once we have established the core arguments behind why capabilities, generality and autonomy might keep increasing, we can look at what risks correspond to these increased levels. We divide risks from AI into three main categories: misuse, misalignment, and systemic risks.",
          "texture": "",
          "reading_time_core": "30 min",
          "reading_time_optional": "15 min",
          "reading_time_appendix": "20 min",
          "resources": {
            "chapter": "/chapters/02",
            "video": "https://www.youtube.com/watch?v=dhr4u-w75aQ",
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1evE1rG91DKBuKlWnqPw45QtPxKBz0GlD_ZYrurNdvN4/edit?usp=sharing"
          },
          "quote": {
            "text": "The greatest risk from AI isn't malice but competence - creating systems that pursue goals misaligned with human values.",
            "author": "Nick Bostrom"
          }
        },
        {
          "id": "ch3", 
          "number": 3,
          "title": "Strategies",
          "subtitle": "What strategies can we develop to mitigate risks?",
          "description": "Building upon the understanding of capabilities and risks, in this chapter we look at concrete strategies for mitigating these risks and making AI development safer. We divide potential mitigation strategies into the same three categories as risks: preventing misuse, addressing alignment, and managing systemic impacts.",
          "texture": "",
          "reading_time_core": "28 min",
          "reading_time_optional": "12 min",
          "reading_time_appendix": "18 min",
          "resources": {
            "chapter": "/chapters/03",
            "video": "https://www.youtube.com/watch?v=iO7Jl4xders",
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1cv0gzwSouDjckYHzV7gYbHPKhJZR6bwbJWgHzEJ604Q/edit?usp=sharing"
          }
        },
        {
          "id": "ch4",
          "number": 4,
          "title": "Governance", 
          "subtitle": "How can we guide safe AI development and deployment?",
          "description": "Previous chapters provided overviews of what AI can do, what could go wrong, and potential mitigation strategies, in this chapter we take a deeper look at how to shape the impact of AI through governance frameworks.",
          "texture": "",
          "reading_time_core": "35 min",
          "reading_time_optional": "18 min",
          "reading_time_appendix": "25 min",
          "resources": {
            "chapter": "/chapters/04",
            "video": "https://www.youtube.com/watch?v=FSKuDqze9es",
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1tp5rpzw_gekjju-UBp8tkbbnQOuA2QzsPF_um8Z4IOU/edit?tab=t.0#heading=h.fo57hwsn3del"
          }
        }
      ]
    },
    {
      "id": "technical-advanced",
      "title": "Technical AI Safety",
      "description": "Technical approaches to ensuring AI systems remain safe and beneficial",
      "chapters": [
        {
          "id": "ch5",
          "number": 5,
          "title": "Evaluations",
          "subtitle": "How do we measure if an AI system is actually safe?",
          "description": "Evaluations is the first chapter in what we consider to be the technical sequence, because it helps us bridge theory and practice by showing how evaluation results can trigger specific governance actions or technical safety protocols.",
          "texture": "",
          "reading_time_core": "22 min",
          "reading_time_optional": "10 min",
          "reading_time_appendix": "15 min",
          "resources": {
            "chapter": "/chapters/05",
            "video": null,
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1T-UU0FBeElX6cvbWYKpVAl3U4ivrQLHA3IdIWqWKuBA/edit?tab=t.0#heading=h.fo57hwsn3del"
          }
        },
        {
          "id": "ch6",
          "number": 6,
          "title": "Specification",
          "subtitle": "Why is telling AI systems what we want so hard?",
          "description": "Evaluations help us figure out what AI systems can do or tend to do, and if our current mitigation measures are enough. The next step is to try and answer - how do we correctly specify what we even want AI to do in the first place?",
          "texture": "",
          "reading_time_core": "27 min",
          "reading_time_optional": "14 min",
          "reading_time_appendix": "16 min",
          "resources": {
            "chapter": "/chapters/06",
            "video": null,
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1JfmzGii5QG6hW8AM5WxzDBVyGc14aLV_Lc_1PkK2ZLc/edit?usp=sharing"
          }
        },
        {
          "id": "ch7",
          "number": 7,
          "title": "Generalization",
          "subtitle": "Why might AIs pursue unintended goals?",
          "description": "In this chapter we turn to how AI systems actually learn and generalize from training. We start by a reminder of what an AI's \"goals\" concretely means, and how people in AI Safety use the term.",
          "texture": "",
          "reading_time_core": "32 min",
          "reading_time_optional": "16 min",
          "reading_time_appendix": "22 min",
          "resources": {
            "chapter": "/chapters/07",
            "video": null,
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1uQooTncb7Hw2NhITtr3S5iGHqT6cvj74c0SZ4Unad_M/edit?usp=sharing"
          }
        },
        {
          "id": "ch8",
          "number": 8,
          "title": "Scalable Oversight",
          "subtitle": "How can we maintain meaningful control when AIs are smarter than us?",
          "description": "Building on the specification problem explored in previous chapters, we look at how we could maintain meaningful human oversight even when tasks start to exceed human ability to iteratively provide feedback.",
          "texture": "",
          "reading_time_core": "29 min",
          "reading_time_optional": "13 min",
          "reading_time_appendix": "19 min",
          "resources": {
            "chapter": "/chapters/08",
            "video": null,
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1DaygDSW0L5dWuJnpSjYPF2XUbW51UoBJsT1cjLYKc2w/edit?usp=sharing"
          }
        },
        {
          "id": "ch9",
          "number": 9,
          "title": "Interpretability",
          "subtitle": "How can we understand what's happening inside AI systems?",
          "description": "Our final chapter examines how we might understand the internal workings of AI models. Progress in interpretability could help address many of the risks and challenges discussed throughout the book.",
          "texture": "",
          "reading_time_core": "26 min",
          "reading_time_optional": "11 min",
          "reading_time_appendix": "17 min",
          "resources": {
            "chapter": "/chapters/09",
            "video": null,
            "audio": null,
            "pdf": null,
            "feedback": "https://forms.gle/ZsA4hEWUx1ZrtQLL9",
            "facilitation": "https://docs.google.com/document/d/1izDWZKR_xB2qj2a8LkbqcnqnjBIC-C7fn-74CIA-m9w/edit?usp=sharing"
          }
        }
      ]
    }
  ]
}
