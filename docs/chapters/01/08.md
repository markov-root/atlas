---
id: 8
title: "Appendix: Expert Opinions"
sidebar_label: "1.8 Appendix: Expert Opinions"
sidebar_position: 9
slug: /chapters/01/08
reading_time_core: "10 min"
reading_time_optional: "1 min"
---
import Iframe from "@site/src/components/chapters/Iframe";
import Video from "@site/src/components/chapters/Video";
import Note from "@site/src/components/chapters/Note";
import Quote from "@site/src/components/chapters/Quote";
import Definition from "@site/src/components/chapters/Definition";

import Figure from "@site/src/components/chapters/Figure";

# Appendix: Expert Opinions

<Video type="youtube" videoId="NqmUBZQhOYw" number="5" label="1.5" caption="Optional video outlining some views that AI experts have on safety and risk." />

<Iframe src="https://ourworldindata.org/grapher/views-ai-impact-society-next-20-years?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="19" label="1.19" caption="Public opinions about AIs impacts on society ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

<Iframe src="https://ourworldindata.org/grapher/views-of-americans-robot-vs-human-intelligence?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="20" label="1.20" caption="Public opinions on machine vs human intelligence ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

## Surveys {#01}

According to a recent survey conducted by AI Impact ([AI Impacts, 2022](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)): *"Expected time to human-level performance dropped 1–5 decades since the 2022 survey. As always, our questions about ‘high-level machine intelligence’ (HLMI) and ‘full automation of labor’ (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year."*

<Figure src="./img/01G_Image_29.png" alt="01G_Image_29.png" number="43" label="1.43"  caption="2024 Survey of AI Experts ([AI Impacts, 2022](https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things))" />

It is also possible to compare the predictions of the same study in 2022 to the current results. It is interesting to note that the community has generally underestimated the speed of progress over the year 2023 and has adjusted its predictions downward. Some predictions are quite surprising. For example, tasks like "Write High School Essay" and "Transcribe Speech" are arguably already automated with ChatGPT and Whisper, respectively. However, it appears that researchers are not aware of these results. Additionally, it is surprising that the forecast for when we are able to build an "AI researcher" has longer timelines than when we are able to build "High-level machine intelligence (all human tasks)".

The median of the 2024 expert survey predicts human-level machine intelligence (HLMI) in 2049.

## Quotes {#02}

Here are many quotes from people regarding transformative AI.

### AI Experts {#02-01}

Note that Hinton, Bengio, and Sutskever are some of the most cited researchers in the field of AI. And that Hinton, Bengio, and LeCun are the recipients of the Turing Award in Deep Learning. Some users on reddit have put together a comprehensive list of publicly stated AI timelines forecasts from famous researchers and industry leaders.

<Quote speaker="Geoffrey Hinton" position="Godfather of modern AI, Turing Award Recipient" date="" source="">

The research question is: how do you prevent them from ever wanting to take control? And nobody knows the answer [...] The alarm bell I'm ringing has to do with the existential threat of them taking control [...] If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further [...] it's as if aliens had landed and people haven't realized because they speak very good English.

</Quote>

<Quote speaker="Yoshua Bengio" position="One of most cited scientists ever, Godfather of modern AI, Turing Award Recipient" date="" source="">

It's very hard, in terms of your ego and feeling good about what you do, to accept the idea that the thing you've been working on for decades might actually be very dangerous to humanity... I think that I didn't want to think too much about it, and that's probably the case for others [...] Rogue AI may be dangerous for the whole of humanity. Banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start.

</Quote>

<Quote speaker="Stuart Russell" position="Co-Author of leading AI textbook, Co-Founder of the Center for Human-Compatible AI" date="" source="">

If we pursue [our current approach], then we will eventually lose control over the machines.

</Quote>

<Quote speaker="Demis Hassabis" position="Co-Founder and CEO of DeepMind" date="" source="">

We must take the risks of AI as seriously as other major global challenges, like climate change. It took the international community too long to coordinate an effective global response to this, and we're living with the consequences of that now. We can't afford the same delay with AI [...] then maybe there's some kind of equivalent one day of the IAEA, which actually audits these things.

</Quote>

<Quote speaker="Dario Amodei" position="Co-Founder and CEO of Anthropic, Former Head of AI Safety at OpenAI" date="" source="">

When I think of why am I scared [...] I think the thing that's really hard to argue with is like, there will be powerful models; they will be agentic; we're getting towards them. If such a model wanted to wreak havoc and destroy humanity or whatever, I think we have basically no ability to stop it.

</Quote>

<Quote speaker="Mustafa Suleyman" position="CEO of Microsoft AI, Co-Founder of DeepMind" date="" source="">

[About a Pause] I don't rule it out. And I think that at some point over the next five years or so, we're going to have to consider that question very seriously.

</Quote>

<Quote speaker="Ilya Sutskever" position="One of the most cited scientists ever, Co-Founder and Former Chief Scientist at OpenAI" date="" source="">

The future is going to be good for the AIs regardless; it would be nice if it would be good for humans as well [...] It's not that it's going to actively hate humans and want to harm them, but it's just going to be too powerful, and I think a good analogy would be the way humans treat animals [...] And I think by default that's the kind of relationship that's going to be between us and AGIs which are truly autonomous and operating on their own behalf.

</Quote>

<Quote speaker="Shane Legg" position="Co-Founder and Chief AGI Scientist at DeepMind" date="" source="">

Do possible risks from AI outweigh other possible existential risks…? It's my number 1 risk for this century [...] A lack of concrete AGI projects is not what worries me, it's the lack of concrete plans on how to keep these safe that worries me.

</Quote>

<Quote speaker="Jan Leike" position="Former co-lead of the Superalignment project at OpenAI" date="" source="">

[After resigning at OpenAI, talking about sources of risks] These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there [...] OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products. We are long overdue in getting incredibly serious about the implications of AGI.

</Quote>

<Quote speaker="Sam Altman" position="Co-Founder and CEO of OpenAI" date="" source="">

[Suggesting about how to ask for a global regulatory body:] "any compute cluster above a certain extremely high-power threshold – and given the cost here, we're talking maybe five in the world, something like that – any cluster like that has to submit to the equivalent of international weapons inspectors" […] I did a big trip around the world this year, and talked to heads of state in many of the countries that would need to participate in this, and there was almost universal support for it.

</Quote>

<Quote speaker="Greg Brockman" position="Co-Founder and Former CTO of OpenAI" date="" source="">

The exact way the post-AGI world will look is hard to predict — that world will likely be more different from today's world than today's is from the 1500s [...] We do not yet know how hard it will be to make sure AGIs act according to the values of their operators. Some people believe it will be easy; some people believe it'll be unimaginably difficult; but no one knows for sure.

</Quote>

<Quote speaker="John Schulman" position="Co-Founder of OpenAI" date="" source="">

[Talking about times near the creation of the first AGI] you have the race dynamics where everyone's trying to stay ahead, and that might require compromising on safety. So I think you would probably need some coordination among the larger entities that are doing this kind of training [...] Pause either further training, or pause deployment, or avoiding certain types of training that we think might be riskier.

</Quote>

<Quote speaker="Jaan Tallinn" position="Co-Founder of Skype, Future of Life Institute" date="" source="">

I've not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It's important that people know lives are being risked [...] One thing that a pause achieves is that we will not push the Frontier, in terms of risky pre-training experiments.

</Quote>

### Academics {#02-02}

<Quote speaker="I. J. Good" position="Cryptologist at Bletchley Park" date="" source="">

An ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

</Quote>

<Quote speaker="Alan Turing" position="Father of Computer Science and AI" date="" source="">

It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers… They would be able to converse with each other to sharpen their wits. At some stage therefore, we should have to expect the machines to take control.

</Quote>

<Quote speaker="Stephen Hawking" position="Theoretical Physicist" date="" source="">

The development of full artificial intelligence could spell the end of the human race [...] It would take off on its own, and re-design itself at an ever increasing rate.

</Quote>

<Quote speaker="Eliezer Yudkowsky" position="AI safety researcher, Co-Founder of Machine Intelligence Research Institute" date="" source="">

I do not expect something actually smart to attack us with marching robot armies with glowing red eyes where there could be a fun movie about us fighting them. I expect an actually smarter and uncaring entity will figure out strategies and technologies that can kill us quickly and reliably and then kill us.

</Quote>

### Tech Entrepreneurs {#02-03}

<Quote speaker="Elon Musk" position="Founder/Co-Founder of OpenAI, Neuralink, SpaceX, xAI, PayPal, CEO of Tesla, CTO of X/Twitter" date="" source="">

AI is a rare case where I think we need to be proactive in regulation than be reactive [...] I think that [digital super intelligence] is the single biggest existential crisis that we face and the most pressing one. It needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely [...] And mark my words, AI is far more dangerous than nukes. Far. So why do we have no regulatory oversight? This is insane.

</Quote>

<Quote speaker="Bill Gates" position="Co-Founder of Microsoft" date="" source="">

Superintelligent AIs are in our future. [...] There's the possibility that AIs will run out of control. [Possibly,] a machine could decide that humans are a threat, conclude that its interests are different from ours, or simply stop caring about us.

</Quote>

### Join Declarations {#02-04}

<Quote speaker="The Bletchley Declaration" position="Multiple Nations & EU" date="2023" source="">

Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.

</Quote>