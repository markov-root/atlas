---
id: 1
title: "State-of-the-Art AI"
sidebar_label: "1.1 State-of-the-Art AI"
sidebar_position: 2
slug: /chapters/01/01
reading_time_core: "9 min"
reading_time_optional: "4 min"
# Pagination control - override automatic sidebar-based pagination
pagination_prev: chapters/01/index
pagination_next: chapters/01/2
---
import Iframe from "@site/src/components/chapters/Iframe";
import Video from "@site/src/components/chapters/Video";
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

import Figure from "@site/src/components/chapters/Figure";

# State-of-the-Art AI

Over the last decade, the field of artificial intelligence (AI) has experienced a profound transformation, largely attributed to the successes in deep learning. This remarkable progress has redefined the boundaries of AI capabilities, challenging many preconceived notions of what machines can achieve. The following sections detail some of these advancements.

<Iframe src="https://ourworldindata.org/grapher/test-scores-ai-capabilities-relative-human-performance?country=Handwriting+recognition~Speech+recognition~Image+recognition~Reading+comprehension~Language+understanding~Predictive+reasoning~Code+generation~Complex+reasoning~General+knowledge+tests~Nuanced+language+interpretation~Math+problem-solving~Reading+comprehension+with+unanswerable+questions&tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="1" label="1.1" caption="Test scores of AI systems on various capabilities relative to human performance. Within each domain, the initial performance of the AI is set to –100. Human performance is used as a baseline, set to zero. When the AI’s performance crosses the zero line, it scored more points than humans ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

Once a benchmark is published, it takes less and less time to solve it. This can illustrate the accelerating progress in AI and how quickly AI benchmarks are "saturating", and starting to surpass human performance on a variety of tasks. ([Our World in Data, 2023](https://ourworldindata.org/grapher/test-scores-ai-capabilities-relative-human-performance?country=Handwriting+recognition~Speech+recognition~Image+recognition~Reading+comprehension~Language+understanding~Predictive+reasoning~Code+generation~Complex+reasoning~General+knowledge+tests~Nuanced+language+interpretation~Math+problem-solving~Reading+comprehension+with+unanswerable+questions))

<Iframe src="https://ourworldindata.org/grapher/domain-notable-artificial-intelligence-systems?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="2" label="1.2" caption="Domain of notable artificial intelligence systems, by year of publication. Describes the specific area, application, or field in which an AI system is designed to operate. An AI system can operate in more than one domain, thus contributing to the count for multiple domains ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

## Language {#01}

<Iframe src="https://ourworldindata.org/grapher/ai-performance-coding-math-knowledge-tests?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="3" label="1.3" caption="Top performing AI systems in coding, math, and language-based knowledge tests. Coding performance is measured with the APPS benchmark; math performance with the MATH benchmark; and language-based knowledge tests with the MMLU benchmark ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

**Language-based tasks.** There have been transformative changes in sequence and language-based tasks, primarily through the development of large language models (LLMs). Early language models in 2018 struggled to construct coherent sentences. The evolution from these to the advanced capabilities of GPT-3 (Generative Pre-Trained Transformer) and ChatGPT within less than 5 years is remarkable. These models demonstrate not only an improved capacity for generating text but also for responding to complex queries with nuanced, common-sense reasoning. Their performance in various question-answering tasks, including those requiring strategic thinking, has been particularly impressive.

In contrast with the text-only GPT-3 and follow-ups, GPT-4 is multimodal: it was trained on both text and images. This means that it can now not only generate text based on images but has also gained some other capabilities. GPT-4 saw an upgraded context window with up to 32k tokens (tokens ≈ words). The short-term memory limit of an LLM can be thought of as the model's ability to retain information from previous tokens within a certain context window. GPT-4 is trained via next-token prediction (autoregressive self-supervised learning). In 2018 GPT-1 was barely able to count to 10, while in 2024 GPT-4 can implement complex programmatic functions among other things.

<Figure src="./img/CAe_Image_2.png" alt="Enter image alt description" number="2" label="1.2" caption="A list of ‘Nowhere near solved’ [...] issues in AI, from ‘A brief history of AI’, published in January 2021 ([Wooldridge, 2021](https://www.amazon.com/Brief-History-Artificial-Intelligence-Where/dp/1250770742)). They also say: ‘At present, we have no idea how to get computers to do the tasks at the bottom of the list’. But everything in the category ‘Nowhere near solved’ has been solved by GPT-4 ([Bubeck et al., 2023](https://arxiv.org/abs/2303.12712)), except human-level general intelligence." />

**Scaling.** Remarkably, GPT-4 is trained using roughly the same methods as GPT-1, 2, and 3. The only significant difference is the size of the model and the data given to it during training. The size of the model has gone from 1.5B parameters to hundreds of billions of parameters, and datasets have become similarly larger and more diverse.

<Figure src="./img/uW1_Image_3.png" alt="Enter image alt description" number="3" label="1.3" caption="How fast is AI Improving? ([AI Digest, 2023](https://theaidigest.org/progress-and-dangers))" />

We have observed that just an expansion in scale has contributed to enhanced performance. This includes improvements in the ability to generate contextually appropriate responses, and highly diverse text across a range of domains. It has also contributed to overall improved understanding, and coherence. Most of those advances in the GPT series come from increasing the size and computation power behind the models, rather than fundamental shifts in architecture or training.

Here are some of the capabilities that have been emerging in the last few years:

- **Few-shot and Zero-shot Learning.** The model's proficiency at understanding and executing tasks with minimal or no prior examples. 'Few-shot' means accomplishing the task after having seen a few examples in the context window, while 'Zero-shot' indicates performing the task without any specific examples ([Anthropic, 2022](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)). This also includes induction capabilities, i.e. identifying patterns and generalizing rules not present in the training, but only present in the current context window ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)).

- **Metacognition.** This refers to the ability to recognize its own knowledge and limitations, for example, being able to know the probability of the truth of something ([Kadavath, 2022](https://arxiv.org/abs/2207.05221)).

- **Theory of Mind.** The capability to attribute mental states to itself and others, which helps in predicting human behaviors and responses for more nuanced interactions ([Kosinski 2023](https://arxiv.org/abs/2302.02083); [Xu et al., 2024](https://arxiv.org/abs/2402.06044)).

- **Tool Use.** Being able to interact with external tools, like using a calculator or browsing the internet, expanding its problem-solving abilities ([Qin et al., 2023](https://arxiv.org/abs/2307.16789)).

- **Self-correction.** The model's ability to identify and correct its own mistakes, which is crucial for improving the accuracy of AI-generated content ([Shinn et al., 2023](https://arxiv.org/abs/2303.11366)).

<Figure src="./img/vR6_Image_4.png" alt="Enter image alt description" number="4" label="1.4" caption="An example of a mathematical problem solved by GPT-4 using Chain of Thought (CoT) ([Bubeck et al., 2023](https://arxiv.org/abs/2303.12712))." />

- **Reasoning.** The advancements in LLMs have also led to significant improvements in the ability to process and generate logical chains of thought and reasoning. This is particularly important in problem-solving tasks where a straightforward answer isn't immediately available, and a step-by-step reasoning process is required. ([Bubeck et al., 2023](https://arxiv.org/abs/2303.12712))

- **Programming ability.** In coding, AI models have progressed from basic code autocompletion to writing sophisticated, functional programs.

- **Scientific & Mathematical ability.** In mathematics, AI's have assisted in the subfield of automatic theorem proving for decades. Today's models continue to assist in solving complex problems. AI can even achieve a gold medal level in the mathematical Olympiad by solving geometry problems ([Trinh et al., 2024](https://www.nature.com/articles/s41586-023-06747-5)).

<Figure src="./img/Fyr_Image_8.png" alt="Enter image alt description" number="5" label="1.5" caption="Note also the large jump from GPT-3.5 to GPT-4 in human percentile on these tests, often from well below the median human to the very top of the human range. ([Aschenbrenner, 2024](https://situational-awareness.ai/from-gpt-4-to-agi/); [OpenAI, 2023](https://arxiv.org/abs/2303.08774)). Keep in mind that the jump from GPT-3 to GPT-4 was in a single year." />

## Image Generation {#02}

The leap forward in image generation is not just in accuracy, but also in the ability to handle complex, real-world images. The latter, particularly with the advent of Generative Adversarial Networks (GANs) in 2014, has shown an astounding rate of progress. The quality of images generated by AI has evolved from simple, blurry representations to highly detailed and creative scenes, often in response to intricate language prompts.

<Figure src="./img/dwX_Image_6.png" alt="Enter image alt description" number="6" label="1.6" caption="An example of state-of-the-art image recognition. The Segment Anything Model (SAM) by Meta’s FAIR (Fundamental AI Research) lab, can classify and segment visual data at highly precise levels. The detection is performed without the need to annotate images. ([Viso AI, 2024](https://viso.ai/deep-learning/segment-anything-model-sam-explained/); [Meta, 2023](https://ai.meta.com/research/publications/segment-anything/))" />

<Figure src="./img/PBp_Image_7.png" alt="Enter image alt description" number="7" label="1.7" caption="An example of the evolution of image generation. At the top left, starting from GANs (Generative Adversarial Networks) to the bottom right, an image from MidJourney V5." />

The rate of progress within a single year alone is quite astounding as is seen from the improvements between the V1 of the MidJourney image generation model in early 2022, to the V6 in December 2023.

<Figure src="./img/wh2_Image_8.png" alt="Enter image alt description" number="8" label="1.8" caption="MidJourney AI image generation over 2022-2023. Prompt: high-quality photography of a young Japanese woman smiling, backlighting, natural pale light, film camera, by Rinko Kawauchi, HDR ([Yap, 2024](https://goldpenguin.org/blog/midjourney-v1-to-v6-evolution/))." />

## Multi & Cross modality {#03}

AI systems are becoming increasingly multimodal. This means that they can process images, text, audio, vision, and robotics using the same model. So they are trained using multiple different "modes" and can translate between them after deployment.

**Cross-modality.** A model is called cross-modal when the input of a model is in one modality (e.g. text) and the output is in another modality (e.g. image). The section on computer vision showed fast progress between 2014 and 2020 in cross-modality. We went from text-to-image models only capable of generating black-and-white pixelated images of faces, to models capable of generating an image of any textual prompt. More examples of cross-modality include OpenAIs Whisper ([Radford et al., 2022](https://arxiv.org/abs/2212.04356)) which is capable of speech-to-text transcription.

**Multi-modality.** A model is called multi-modal when both the inputs and outputs of a model can be in more than one modality. E.g. audio-to-text, video-to-text, text-to-image, etc…

<Figure src="./img/OCe_Image_12.png" alt="Enter image alt description" number="9" label="1.9" caption="Image-to-text and text-to-image multimodality from the Flamingo model ([Alayrac et al., 2022](https://arxiv.org/abs/2204.14198))." />

DeepMind’s 2022 Flamingo model, could be "*rapidly adapted to various image/video understanding tasks*" and "*is also capable of multi-image visual dialogue*". ([Alayrac et al., 2022](https://arxiv.org/abs/2204.14198)) Similarly, DeepMind’s 2022 Gato model, was called a "Generalist Agent". It was a single network with the same weights which could "*play Atari, caption images, chat, stack blocks with a real robot arm, and much more*". ([Reed et al., 2022](https://arxiv.org/abs/2205.06175)) Continuing this trend, DeepMind’s 2023 Google Gemini model could be called a Large Multimodal Model (LMM). The paper described Gemini as "*natively multimodal*" and claimed to be able to "*seamlessly combine their capabilities across modalities (e.g. extracting information and spatial layout out of a table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its state-of-art-performance in math and coding)*"([Google, 2024](https://arxiv.org/abs/2312.11805))

## Robotics {#04}

<Iframe src="https://ourworldindata.org/grapher/annual-professional-service-robots-installed-by-area?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="4" label="1.4" caption="Annual professional service robots installed globally, by application area. Professional service robots are semi- or fully autonomous machines that perform useful tasks in a professional setting outside of industrial applications, such as in cleaning or medical surgery. Consumer service robots are not included. ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))" />

The field of robotics has also been progressing alongside artificial intelligence. In this section, we provide a couple of examples where these two fields are merging, highlighting some robots using inspiration from machine learning techniques to make advancements.

<Figure src="./img/VZl_Image_9.png" alt="Enter image alt description" number="10" label="1.10" caption="Researchers used Model-Free Reinforcement Learning to automatically learn quadruped locomotion in only 20 minutes in the real world instead of in simulated environments. The Figure shows examples of learned gaits on a variety of real-world terrains ([Smith et al., 2022](https://arxiv.org/abs/2208.07860))." />

<Iframe src="https://ourworldindata.org/grapher/annual-industrial-robots-installed?tab=chart" width="100%" height="600px" loading="lazy" allow="web-share; clipboard-write" frameBorder="0" number="5" label="1.5" caption="Annual industrial robots installed in top five countries. Industrial robots are automated, reprogrammable machines that perform a variety of tasks in industrial settings ([Giattino et al., 2023](https://ourworldindata.org/artificial-intelligence))." />

**Advances in robotics.** At the forefront of robotic advancements is PaLM-E, a general-purpose, embodied model with 562 billion parameters that integrates vision, language, and robot data for real-time manipulator control and excels in language tasks involving geospatial reasoning ([Driess et al., 2023](https://arxiv.org/abs/2303.03378)).

Simultaneously, developments in vision-language models have led to breakthroughs in fine-grained robot control, with models like RT-2 showing significant capabilities in object manipulation and multimodal reasoning. RT-2 demonstrates how we can use LLM-inspired prompting methods (chain-of-thought), to learn a self-contained model that can both plan long-horizon skill sequences and predict robot actions ([Brohan et al., 2023](https://arxiv.org/abs/2307.15818)).

Mobile ALOHA is another example of combining modern machine learning techniques with robotics. Trained using supervised behavioral cloning, the robot can autonomously perform complex tasks "*such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet*" ([Fu et al., 2024](https://arxiv.org/abs/2401.02117)). Such advancements not only demonstrate the increasing sophistication and applicability of robotic systems but also highlight the potential for further groundbreaking developments in autonomous technologies.

<Figure src="./img/3A4_Image_10.png" alt="Enter image alt description" number="11" label="1.11" caption="DeepMinds RT-2 can both plan long-horizon skill sequences and predict robot actions using inspiration from LLM prompting techniques (chain-of-thought) ([Brohan et al., 2023](https://arxiv.org/abs/2307.15818))." />

<Video type="youtube" videoId="Sq1QZB5baNw" number="1" label="1.1" caption="Optional video showcasing one example of the current state of robotics." />

## Playing Games {#05}

**AI and board games.** AI has made continuous progress in game playing for decades. Starting from AIs beating the world champion at chess in 1997, Scrabble in 2006 to DeepMind’s AlphaGo in 2016 ([DeepMind, 2016](https://www.deepmind.com/research/highlighted-research/alphago)), which was good enough to defeat the world champion in the game of Go, a game assumed to be notoriously difficult for AI. Within a year, the next model AlphaGo Zero trained through self-play had mastered multiple games of Go, chess, and shogi reaching a superhuman level after less than three days of training ([Silver et al., 2017](https://arxiv.org/abs/1712.01815)).

**AI and video games.** We started using machine learning techniques on simple Atari games in 2013 ([Mnih et al. 2013](https://arxiv.org/abs/1312.5602)). By 2019, OpenAI Five defeated the world champions at DOTA2 ([OpenAI, 2019](https://openai.com/research/openai-five-defeats-dota-2-world-champions)), while in the same year, DeepMind’s AlphaStar beat professional esports players at StarCraft II ([DeepMind, 2019](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/)). Both these games require thousands of actions in a row at a high number of actions per minute. In 2020 DeepMind MuZero model, described as "*a significant step forward in the pursuit of general-purpose algorithms*" ([DeepMind, 2020](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules)), was capable of playing Atari games, Go, chess, and shogi without even being told the rules.

In recent years, AI's capability has extended to open-ended environments like Minecraft, showcasing an ability to perform complex sequences of actions. In strategy games, Meta’s Cicero displayed intricate strategic negotiation and deception skills in natural language for the game Diplomacy ([Bakhtin et al., 2022](https://arxiv.org/abs/2210.05492)).

<Figure src="./img/KMJ_Image_11.png" alt="Enter image alt description" number="12" label="1.12" caption="A map of diplomacy and the dialog box where the AI negotiates ([DiploStrats (YouTube), 2022](https://www.youtube.com/watch?v=u5192bvUS7k&t=2216s))." />

<Note title="Example of Voyager: Planning and Continuous Learning in Minecraft with GPT-4" collapsed={true}>

Voyager ([Wang et al., 2023](https://arxiv.org/abs/2305.16291)) stands as a particularly impressive example of the capabilities of AI in continuous learning environments. This AI is designed to play Minecraft, a task that involves a significant degree of planning and adaptive learning. What makes Voyager so remarkable is its ability to learn continuously and progressively within the game's environment, using GPT-4 contextual reasoning abilities to plan and write the code necessary for each new challenge. Starting from scratch in a single game session, Voyager initially learns to navigate the virtual world, engage and defeat enemies, and remember all these skills in its long-term memory. As the game progresses, it continues to learn and store new skills, leading up to the challenging task of mining diamonds, a complex activity that requires a deep understanding of the game mechanics and strategic planning. The ability of Voyager to integrate new information continuously and utilize it effectively showcases the potential of AI in managing complex, changing environments and performing tasks that require a long-term buildup of knowledge and skills.

<Figure src="./img/ccN_Image_12.png" alt="Enter image alt description" number="13" label="1.13" caption="Voyager discovers new Minecraft items and skills continually by self-driven exploration, significantly outperforming the baselines ([Wang et al., 2023](https://arxiv.org/abs/2305.16291))." />

</Note>