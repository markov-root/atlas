---
id: 1
title: "What is Interpretability ?"
sidebar_label: "9.1 What is Interpretability ?"
sidebar_position: 2
slug: /chapters/09/01
reading_time_core: "3 min"
reading_time_optional: "1 min"
---
import Video from "@site/src/components/chapters/Video";
import Footnote, { FootnoteRegistry } from "@site/src/components/chapters/Footnote";
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

import Figure from "@site/src/components/chapters/Figure";

# What is Interpretability ?

Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter—and the broader context of AI safety—**mechanistic interpretability (mech interp)** is the primary focus ([Ras et al., 2020](https://arxiv.org/abs/2004.14545), [Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148)).<Footnote id="footnote_interp_overview" number="1" text="For an overview of the broader interpretability landscape see ([Ras et al., 2020](https://arxiv.org/abs/2004.14545); [Ali et al., 2023](https://www.sciencedirect.com/science/article/pii/S1566253523001148))" />



<Video type="youtube" videoId="UGO_Ehywuxc" number="3" label="9.3" caption="Optional video explanation of mechanistic interpretability." />

## **Mechanistic Interpretability Mechanistic Interpretability: The Bottom-Up Approach.** Mechanistic interpretability seeks to reverse-engineer neural networks to uncover how their internal components—such as neurons, weights, and layers—work together to process information. This approach starts at the lowest level of abstraction and builds understanding piece by piece: this is why it’s considered a bottom-up approach. By analyzing these basic components, we hope we can piece together how the network processes information and makes decisions. {#01}

For example, mechanistic interpretability could explain how a neural network recognizes objects in an image or generates language, down to the contributions of individual neurons or attention heads. The hope is that this level of detail will allow researchers to diagnose and potentially fix unwanted behaviors in AI systems.

**Other Approaches to Interpretability.** While mechanistic interpretability is a strong focus in AI safety, it is not the only approach. Other methods provide complementary perspectives:

- **Concept-Based Interpretability:** Contrarily to mechanistic interpretability, concept-based interpretability takes a** top-down approach:** instead of analyzing neurons or weights on a granular level, it focuses on understanding how the network manipulates **high-level concepts** ([Belinkov, 2022](https://aclanthology.org/2022.cl-1.7/)). For instance, *representation engineering *—a concept-based research agenda— explores how models encode concepts like "honesty" and how those representations can be adjusted to produce more honest outputs ([Zou et al., 2023](https://www.semanticscholar.org/paper/Representation-Engineering%3A-A-Top-Down-Approach-to-Zou-Phan/aac3469581061cd5b46440c3eeca91c385d54ccf)).

- **Developmental Interpretability:** This approach examines how model capabilities and internal representations **evolve during training.** By understanding the emergence of behaviors or knowledge over time, researchers hope they will be able to identify the emergence of certain capabilities and prevent unwanted ones from developing ([Hoogland et al., 2023](https://www.lesswrong.com/s/SfFQE8DXbgkjk62JK/p/TjaeCWvLZtEDAS5Ex)).

- **Behavioral Interpretability:** Unlike the previous approaches, behavioral interpretability studies **input-output relationships without delving into the internal structure of models.**

<Figure src="./img/p8v_Image_1.png" alt="Enter image alt description" number="1" label="9.1" caption="A visual classification of interpretability techniques ([Bereska & Gavves, 2024](https://arxiv.org/abs/2404.14082))." />

**Why Mechanistic Interpretability Matters for AI Safety.** Mechanistic interpretability is a strong focus in AI safety because it provides a level of precision that other approaches do not. Behavioral interpretability, for instance, offers insights into how a model behaves by studying input-output relationships, but it cannot reveal how its internal structure leads to its decisions. To prevent AI models from making harmful decisions or ensuring alignment with human values, we need to understand why models make certain decisions, and potentially steer the decision-making process.

For instance, by pinpointing where harmful concepts—such as instructions for cyberattacks—are stored within a model, mechanistic interpretability tools could help erase or modify those concepts without degrading the model’s overall capabilities.

<FootnoteRegistry title="Footnotes" />
