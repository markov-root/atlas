---
id: 4
title: "Automating and Scaling Interpretability"
sidebar_label: "9.4 Automating and Scaling Interpretability"
sidebar_position: 5
slug: /chapters/09/04
reading_time_core: "1 min"
# Pagination control - override automatic sidebar-based pagination
pagination_prev: chapters/09/3
pagination_next: chapters/09/5
---
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

# Automating and Scaling Interpretability

State-of-the-art models now contain hundreds of billions of parameters and thousands of interconnected layers, making manual inspection of model components infeasible. Mechanistic interpretability aims to analyze how individual elements—like attention heads, neurons, features, or entire layers—interact to produce specific behaviors. However, as models scale, manual approaches like activation pathing for circuit discovery, subgraph study, and subsequent explanation generation ([Wang et al., 2023](https://openreview.net/forum?id=NpsVSN6o4ul)), become infeasible to use. This is why developing mechanistic interpretability methods that can scale is essential.

**Research in scalable interpretability, such as Automated Circuit DisCovery (ACDC) **attempts to introduce algorithms that automate the process of finding circuits within a transformer.