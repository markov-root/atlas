---
id: 6
title: "Critiques"
sidebar_label: "9.6 Critiques"
sidebar_position: 7
slug: /chapters/09/06
reading_time_core: "1 min"
# Pagination control - override automatic sidebar-based pagination
pagination_prev: chapters/09/5
pagination_next: null
---
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

# Critiques

While interpretability offers potential value in understanding complex machine learning models, it faces several critical limitations that restrict its practical impact. Below are the main challenges that restrict interpretability’s usefulness in ensuring AI safety:

- **Limited practical use:** Interpretability tools and techniques rarely provide actionable insights for real-world applications, especially in industry.

- **Issues with Enumerative Safety: **Enumerative safety—the idea of analyzing every feature within a model to detect dangerous elements—faces inherent issues. High-level behaviors, not individual features, often drive risk. Focusing on isolated “risky” neurons or components can miss the broader capabilities that are more likely to cause harm.

- **Improved capabilities: **Although interpretability is intended to enhance safety, it can also unintentionally improve model performance in ways that might increase risk. For example, better insights into a model’s behavior can sometimes make it more capable without necessarily making it safer.

- **Alternative approaches can be more effective: **In many cases, tasks that interpretability aims to address, such as detecting and preventing undesirable behavior, are better achieved through other strategies, like evaluations, red-teaming, or fine-tuning.