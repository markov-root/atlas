---
id: 10
title: "Appendix: Requirements for ASI Alignment"
sidebar_label: "3.10 Appendix: Requirements for ASI Alignment"
sidebar_position: 11
slug: /chapters/03/10
reading_time_core: "3 min"
reading_time_optional: "5 min"
---
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

# Appendix: Requirements for ASI Alignment

**ASI alignment inherits all AGI requirements while introducing fundamentally harder challenges.** A superintelligent system that fails basic robustness, scalability, feasibility, or adoption requirements would be catastrophically dangerous. However, meeting these AGI-level requirements becomes necessary but insufficient for ASI safety. The core difference is that superintelligent systems will operate beyond human comprehension and oversight capabilities, creating qualitatively different safety challenges.

**Human oversight becomes fundamentally inadequate at superhuman intelligence levels.** When AI systems surpass human capabilities across most domains, we lose our ability to evaluate their reasoning, verify their outputs, or provide meaningful feedback ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). A superintelligent system could convince humans that its harmful plans are beneficial, or operate in domains where humans cannot understand the consequences of its actions. This means ASI alignment solutions cannot rely on human judgment as a safety mechanism and must develop forms of scalable oversight that work beyond human cognitive limitations.

**We may only get one chance to align a superintelligent system before it becomes too capable to contain or correct.** This "one-shot" requirement stems from the potential for rapid capability gains that could make a misaligned system impossible to shut down or modify ([Soares, 2022](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization); [Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). Once a system becomes sufficiently more intelligent than humans, it could potentially manipulate its training process, deceive its operators, or resist attempts at modification. However, this requirement depends on contested assumptions about takeoff speeds - some researchers argue for more gradual development that would allow iteration and correction ([Christiano, 2022](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer)). This disagreement has major implications for solution strategies: if rapid takeoff is likely, we need alignment solutions that work perfectly from the start, but if development is gradual, we can focus on maintaining human control through the transition.

**Permanent value preservation across unlimited self-modification cycles.** Superintelligent systems may recursively improve their own capabilities, potentially rewriting their core algorithms, goal structures, and reasoning processes entirely ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). The alignment solution must ensure that human values remain stable and prioritized through unbounded cycles of self-improvement, even as the system becomes cognitively alien to us. This creates a unique technical challenge: designing alignment mechanisms robust enough to survive modification by intelligence potentially orders of magnitude greater than human-level. Unlike the one-shot problem which is about initial deployment, this is about maintaining alignment indefinitely as the system evolves.

**Control over systems with civilizational-scale power and influence.** A superintelligent system will likely have enormous technological capabilities and influence over human civilization - potentially developing advanced nanotechnology, novel manipulation techniques, or reshaping institutions and culture over time ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). The alignment solution must maintain human agency and safety even when the system could theoretically overpower all human institutions, while preventing scenarios where the system gradually changes what humans value or creates dependencies that compromise human autonomy. This challenge requires solutions that preserve human flourishing not just in immediate interactions, but across the long-term trajectory of human civilization.

<Note title="Pivotal acts" collapsed={true}>

**Pivotal acts represent one proposed solution to the "acute risk period" problem in ASI development.** The core concern is that we may enter a period where multiple actors are capable of developing superintelligent AI, but only one needs to be misaligned or reckless to cause global catastrophe ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). Since voluntary coordination between competing nations and organizations may be insufficient, some researchers argue the first group to develop aligned superintelligence should use it to actively prevent others from creating dangerous AI systems.

**Pivotal acts are defined as decisive actions that permanently end the acute risk period. **These actions must be powerful enough to prevent any other actor from developing unaligned superintelligence, potentially through technological interventions that disable global computing infrastructure, establish unbreakable international agreements, or develop other mechanisms that make uncontrolled AI development physically impossible ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). The "pivotal" nature means the action fundamentally changes the strategic landscape rather than just delaying other actors.

**The argument for pivotal acts stems from coordination failures and competitive pressures.** Even if most AI developers prioritize safety, competitive dynamics between nations and companies create pressure to deploy systems quickly rather than safely ([Yudkowsky, 2022](https://intelligence.org/2022/06/10/agi-ruin/)). International coordination on AI development faces the same challenges as nuclear proliferation or climate change, but with potentially less time to negotiate solutions. Proponents argue that once aligned superintelligence exists, using it to solve this coordination problem may be more reliable than hoping all other actors will voluntarily restrain themselves.

Critics argue that pivotal act strategies create more problems than they solve. Planning to perform pivotal acts militarizes AI development and incentivizes unilateral action, potentially making the acute risk period more dangerous rather than safer ([Critch, 2022](https://www.alignmentforum.org/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes)). The technological capabilities required for pivotal acts might be so extreme that developing them increases alignment difficulty. Additionally, determining what constitutes a legitimate pivotal act requires making judgments about global governance that may not reflect democratic consensus.

Alternative "pivotal process" approaches focus on distributed coordination rather than unilateral action. Instead of single decisive interventions, these strategies involve using aligned AI to improve human decision-making, demonstrate risks convincingly, develop better governance mechanisms, or consume resources that unaligned AI might use for rapid scaling ([Critch, 2022](https://www.alignmentforum.org/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes); [Christiano, 2022](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer)). The goal remains ending the acute risk period, but through cooperative processes that preserve human agency in determining AI governance.

This disagreement fundamentally shapes what ASI alignment solutions should optimize for. Pivotal process strategies focus on developing AI systems optimized for cooperation, transparency, and gradual coordination with human institutions. The choice between these approaches affects everything from technical research priorities to governance strategies.

</Note>

<Note title="The Strawberry Problem and requirements for ASI alignment" collapsed={true}>

**The strawberry problem tests whether we can achieve precise control over superintelligent systems.** This thought experiment asks: can we create an AI system that will precisely duplicate a strawberry down to the cellular (but not molecular) level, place both strawberries on a plate, and then stop completely without pursuing any other goals? This seemingly simple task helps understand the different debates about what AGI and ASI alignment solutions should aim to achieve ([Soares, 2022](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization)). Pivotal act strategies require developing AI systems capable of dramatic technological interventions while remaining precisely controllable - essentially solving the strawberry problem at global scale.

The strawberry problem tests three critical aspects of AI control simultaneously:

- **Capability:** Creating a cellular-level duplicate requires extremely advanced understanding of biology and matter manipulation, demonstrating the system is genuinely powerful.

- **Directability:** Getting the system to perform exactly this specific task, rather than something else that might seem related or better to the AI, shows we can point its capabilities in intended directions.

- **Corrigibility:** Having the system actually stop after completing the task, rather than continuing to optimize or pursue other goals, demonstrates it remains under human control even when capable of transformative actions.

**Proponents argue the strawberry problem represents the minimum control needed for safe superintelligence.** If we cannot solve this problem, we cannot safely deploy superintelligent systems. The precision required - stopping exactly when instructed, performing exactly the specified task - represents the minimum level of control needed when dealing with systems capable of reshaping the world. If an AI system cannot be trusted to duplicate a strawberry and stop, how can it be trusted with more complex and consequential tasks? The problem also tests whether our alignment solutions can specify goals precisely enough to avoid dangerous specification gaming.

**Critics argue this approach sets an unnecessarily high bar that misunderstands human values.** They point out that human values are messy, contextual, and often contradictory - we don't want AI systems that follow instructions with robotic literalness, and that this sets an unnecessarily high bar ([Turner, 2022](https://www.alignmentforum.org/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into); [Pope, 2023](https://www.alignmentforum.org/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky)). Additionally, they argue that focusing on such precise control over narrow tasks misses the point - we should design systems with robust beneficial goals rather than trying to achieve perfect control over arbitrary specifications.

**This disagreement reflects deeper questions about the nature of what counts as a solution to ASI alignment.** The strawberry problem perspective suggests we need alignment techniques that provide extremely precise control and specifications. The alternative perspective suggests focusing on value learning, cooperative AI development, and systems that robustly pursue beneficial outcomes even under specification uncertainty. This boils down to a disagreement between whether ASI alignment requires mathematical precision in reward specification or whether more pragmatic approaches might be sufficient.

</Note>