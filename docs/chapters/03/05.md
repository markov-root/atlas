---
id: 5
title: "Systemic Strategies"
sidebar_label: "3.5 Systemic Strategies"
sidebar_position: 6
slug: /chapters/03/05
section_description: "Why does building safe AI benefit from a shared safety culture?"
reading_time_core: "14 min"
reading_time_optional: "11 min"
# Pagination control - override automatic sidebar-based pagination
pagination_prev: chapters/03/4
pagination_next: chapters/03/6
---
import Footnote, { FootnoteRegistry } from "@site/src/components/chapters/Footnote";
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

import Figure from "@site/src/components/chapters/Figure";

# Transversal Strategies

AI Safety is a socio technical problem that requires a socio technical solution. Ensuring AI safety also requires robust systemic approaches. These encompass the governance structures, organizational practices, and cultural norms that shape AI development and deployment. Technical safety measures can be undermined by inadequate governance, poor security practices within labs, or a culture that prioritizes speed over caution. This section examines strategies aimed at building safety into the broader ecosystem surrounding AI.

Addressing the systemic risks posed by AI is not easy. It requires ongoing, multidisciplinary collaboration and solving complex coordination games. The fact that responsibility for the problem is so diverse makes it difficult to make the solutions actionable.

<Figure src="./img/sZG_Image_20.png" alt="Enter image alt description" number="20" label="3.20" caption="An illustration of a framework that we think is robustly good to manage risks.

AI Risks are too numerous and too heterogeneous. To address these risks, we need an adaptive framework that can be robust and evolve as AI advances." />

## Defense Acceleration (d/acc) {#01}

**Defense acceleration (d/acc) is a strategic approach of prioritizing technologies that strengthen defense and social resilience against AI risks.** Defense acceleration, or d/acc, emerged as a strategic approach in 2023 as a middle path between unrestricted acceleration (effective accelerationism (e/acc)) and techno pessimists/doomers ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html); [Buterin, 2025](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)). Rather than relying solely on restricting access to potentially dangerous capabilities, d/acc proposes only accelerating technologies that inherently favor defense over offense, thereby making society more resilient to various threats including AI misuse by humans or misaligned behavior by AI themselves. D/acc can be understood by thinking about the question - if AI takes over the world (or disempowers humans), how would it do so?

- **It hacks our computers → accelerate cyber-defense: **Employ AI to identify and patch vulnerabilities, monitor systems for intrusions, and automate security responses.

- **It creates a super-plague → accelerate bio-defense:** Developing technologies to detect, prevent, and treat biological threats, including advanced air filtration systems, rapid diagnostic tools, far-UVC irradiation to safely sterilize occupied spaces, and decentralized vaccine production capabilities.

- **It convinces us (either to trust it, or to distrust each other) → accelerate info-defense: **Create systems that help validate information accuracy and detect misleading content without centralized arbiters of truth, such as blockchain-secured provenance tracking and community-verified fact-checking systems like Twitter's Community Notes

- **It disrupts infrastructure → accelerate physical-defense:** Creating resilient infrastructure that can withstand disruptions, such as distributed energy generation through household solar, battery storage systems, and advanced manufacturing techniques that enable local production of essential goods.

<Figure src="./img/Ell_Image_21.png" alt="Enter image alt description" number="21" label="3.21" caption="An illustration of different things that we could work on in the d/acc philosophy to prioritize defensive technologies ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html))." />

<Note title="A concrete example: AI for Cyberdefense" collapsed={true}>

A key application is using AI to improve cybersecurity. Powerful AI could potentially automate vulnerability detection, monitor systems for intrusions, manage fine-grained permissions more effectively than humans, or displace human operators from security-critical tasks ([Shlegeris, 2024](https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically)). While current models may not yet be reliable enough, the potential exists for AI to significantly bolster cyber defenses against both conventional and AI-driven attacks ([Jade Hill, 2024](https://abnormalsecurity.com/blog/offensive-ai-defensive-ai); [Schlegeris, 2024](https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically)) outlines four promising strategies for using AI to enhance security:

- Comprehensive monitoring of human actions with AI flagging suspicious activities

- Trust displacement where AI handles sensitive tasks instead of humans

- Fine-grained permission management that would be too labor-intensive for humans

- AI-powered investigation of suspicious activities.

These approaches could dramatically reduce insider threats and data exfiltration risks, potentially making computer security "radically easier" when powerful AI becomes available, even if there is substantial uncertainty on the robustness of such techniques.

</Note>

**D/acc represents three interconnected principles: defensive, decentralized, and differential technological development.** The "d" in d/acc stands for:

- **Defensive: **Prioritizing technologies that make it easier to protect against threats than to create them. Purely restrictive approaches face inherent limitations - they require global coordination, create innovation bottlenecks, and risk concentrating power in the hands of those who control access.

- **Differential: **Accelerating beneficial technologies while being more cautious about those with harmful potential. The order in which technology is developed matters a lot. By differentially accelerating defensive technologies (like advanced cybersecurity measures) ahead of potentially dangerous capabilities (like autonomous hacking systems), we create protective layers before they're urgently needed.

- **Decentralized:** We can strengthen resilience by eliminating single points of failure. Centralized control of powerful AI capabilities creates vulnerabilities to technical failures, adversarial attacks, and institutional capture ([Cihon et al., 2020](https://arxiv.org/abs/2001.03573)). Decentralized approaches distribute both capabilities and governance across diverse stakeholders, preventing unilateral control over transformative technologies.

<Figure src="./img/8pT_Image_22.png" alt="Enter image alt description" number="22" label="3.22" caption="Mechanisms by which differential technology development can reduce negative societal impacts ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html))." />

**The effectiveness of d/acc depends on maintaining favorable offense-defense balances. **The feasibility of d/acc as a strategy hinges on whether defensive technologies can outpace offensive capabilities across domains. Historical precedents are mixed - some fields like traditional cybersecurity often favor defenders who can patch vulnerabilities, while others like biosecurity traditionally favor attackers who need fewer resources to create threats than defenders need to counter them. The key challenge for d/acc implementation lies in identifying and supporting technologies that shift these balances toward defense ([Bernardi, 2024](https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration); [Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html)).

<Note title="Actionable strategies aligned with the d/acc philosophy" collapsed={true}>

**D/acc complements rather than replaces other safety approaches.** Unlike competing frameworks that may view restrictions and safeguards as impediments to progress, d/acc recognizes their value while addressing their limitations. Model safeguards remain essential first-line defenses, but d/acc builds additional safety layers when those safeguards fail or are circumvented. Similarly, governance frameworks provide necessary oversight, but d/acc reduces dependency on perfect regulation by building technical resilience that functions even during governance gaps.

**Actionable governance and policy approaches to d/acc.** Policy interventions can help create structured frameworks for defensive acceleration. Some examples of work in governance that supports the d/acc philosophy includes:

- **Information sharing frameworks:** Establish mandatory incident reporting and information sharing protocols between AI developers and security agencies ([Bernardi, 2024](https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration)).

- **Defender-first access:** Implement policies that grant security researchers privileged early access to advanced AI capabilities before general release ([Bernardi, 2024](https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration)).

- **Defense acceleration funds:** Create dedicated funding mechanisms for defensive technologies to address market failures where public good technologies lack sufficient private investment despite their social value ([Bernardi, 2024](https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration); [Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html)).

- **Staged capability deployment:** Require phased rollouts of advanced AI capabilities with monitoring periods between stages ([Bernardi, 2024](https://airesilience.substack.com/p/a-policy-agenda-for-defensive-acceleration)).

**Actionable technological and research approaches to d/acc.** We can differentially advance technological progress in many different domains to favor defense against catastrophic risks. Here are just a couple of examples:

- **Advanced air quality systems:** Develop integrated systems that detect, filter, and neutralize airborne pathogens in real-time. These technologies provide passive protection against both natural pandemics and engineered bioweapons without requiring behavioral changes or perfect compliance ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html)).

- **Privacy-preserving computation:** Advance cryptographic techniques like zero-knowledge proofs, homomorphic encryption, and secure multi-party computation. These methods enable verification and secure collaboration without exposing sensitive information, fundamentally shifting security-privacy tradeoffs ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html)).

- **Resilient infrastructure:** Create decentralized, self-sufficient systems for energy, communication, and supply chains that can operate during disruptions. This includes technologies like mesh networks, localized manufacturing, and distributed energy generation that maintain critical functions even when centralized systems fail ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html); [Buterin, 2025](https://vitalik.eth.limo/general/2025/01/05/dacc2.html)).

- **Collaborative verification systems:** Implement cross-spectrum information validation platforms similar to Community Notes that identify misinformation through consensus across viewpoint diversity. These systems enable communities to self-regulate information quality without centralized arbiters of truth ([Buterin, 2023](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html)).

<Figure src="./img/2Qx_Image_23.png" alt="Enter image alt description" number="23" label="3.23" caption="An extended model showcasing how many different types and layers of defensive technologies can interact ([Buterin, 2025](https://vitalik.eth.limo/general/2025/01/05/dacc2.html))" />

</Note>

## Defense in Depth {#02}

Effective AI safety also depends heavily on the internal practices and structures within the organizations developing AI. Accidents are hard to avoid, even when the incentive structure and governance try to ensure that there will be no problems. For example, even today, there are still accidents in the aerospace industry.

<Figure src="./img/NpS_Image_24.png" alt="Enter image alt description" number="24" label="3.24" caption="The Swiss cheese model shows how technical factors can improve organizational safety. Multiple layers of defense compensate for each other’s individual weaknesses, leading to a low overall level of risk ([Hendrycks et al., 2023](https://arxiv.org/abs/2306.12001))." />

To solve those problems, a Swiss cheese model might be effective— no single solution will suffice, but a layered approach can significantly reduce risks. The Swiss cheese model is a concept from risk management, widely used in industries like healthcare and aviation. Each layer represents a safety measure, and while individually they might have weaknesses, collectively they form a strong barrier against threats. Organizations should also follow safe design principles ([Hendrycks & Mazeika, 2022](https://arxiv.org/abs/2206.05862)), such as defense in depth and redundancy, to ensure backup for every safety measure, among others.

**Many solutions can be imagined to reduce those risks, even if none is perfect.** The first step could be commissioning external red teams to identify hazards and improve system safety. This is what OpenAI did with METR to evaluate GPT-4. However AGI labs also need an internal audit team for risk management. Just like banks have risk management teams, this team needs to be involved in the decision processes, and key decisions should involve a chief risk officer to ensure executive accountability. One of the missions of the risk management team could be, for example, designing pre-set plans for managing security and incidents.

**Limitations of Systemic Approaches.** Defense-in-depth might still fail against sufficiently novel threats or determined adversaries, especially in the case of ASIs. Similarly, gradual disempowerment scenarios, where human control erodes incrementally through economic or cultural shifts driven by AI, may not be adequately addressed by current governance frameworks focused on specific harms or capabilities.

## AI Governance {#03}

The pursuit of more and more powerful AI, much like the nuclear arms race of the Cold War era, represents a trade-off between safety and the competitive edge nations and corporations seek for power and influence. This competitive dynamic increases global risk. To mitigate this problem, we can try to act at the source of it, namely, the redesign of economic incentives to prioritize long-term safety over short-term gains. This can mainly be done via international governance.

Effective AI governance aims to achieve two main objectives:

1. Gaining time. Time and resources for solution development to ensure that sufficient time and resources are allocated for identifying and implementing safety measures

2. Enforcing solutions. Enhanced coordination to increase the likelihood of widespread adoption of safety measures through global cooperation. AI risks are multifaceted, necessitating regulations that encourage cautious behavior among stakeholders and timely responses to emerging threats.

**Designing high-level better incentives**

Aligning economic incentives with safety goals is a key challenge. Currently, strong commercial pressures can incentivize rapid capability development, potentially at the expense of safety research or cautious deployment. Mechanisms to reward safety or penalize recklessness are needed to avoid negative externalities:

- **Reshaping the race via a centralized development.** For example, Yoshua Bengio et al. propose creating a secure facility akin to CERN for physics, where the development of potentially dangerous AI technologies can be tightly controlled ([Bengio, 2023](https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/)). This measure is highly non consensual. We already explored this solution in strategy "World Coordination" ASI safety, in the section ASI safety, but this could also be valid for many domains of safety.

- **Windfall clauses and benefit sharing.** Implementing agreements to share the profits between the different labs generated from AGI would mitigate the race to AI supremacy by ensuring collective benefit from individual successes. <Footnote id="footnote_windfall" number="1" text="For example, in the pharmaceutical industry for drug development, companies sometimes enter into co-development and profit-sharing agreements to share the risks and rewards of bringing a new drug to market. For example, in 2014, Pfizer and Merck entered into a global alliance to co-develop and co-commercialize an anti-PD-L1 antibody for the treatment of multiple cancer types." />

- **Implementing a correct governance of AGI companies.** It is important to examine the governance structures of AGI labs. For example, being a non-profit and having a mission statement that makes it clear that the goal is not to maximize revenue, but to ensure that the development of AI benefits all of humanity, is an important first step. Also, the board needs to have teeth.

- **Legal liability for AI developers.** Establishing clear legal responsibilities for AI developers regarding misuse or accidents might realign the incentives. For example, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) could have enabled the Attorney General to bring civil suits against developers who cause catastrophic harm or threaten public safety by neglecting the requirements. The bill (which was vetoed by the governor in 2024) only addressed extreme risks from these models, including: cyberattacks causing over 500 million dollars in damage, autonomous crime causing 500 million dollars in damage, and the creation of chemical, biological, radiological, or nuclear weapons using AI. Note that compared with the AI Act and it’s code of practice, SB1047 does not specify in details the steps needed to assure we avoid catastrophes, it only targets the outcome and not really the process.



**Proposed International AI Governance Mechanisms Several mechanisms have been proposed to establish clear boundaries and rules for AI development internationally.** These include implementing temporary moratoriums on high-risk AI systems, enforcing legal regulations like the EU AI Act, and establishing internationally agreed-upon "Red Lines" that prohibit specific dangerous AI capabilities, such as autonomous replication or assisting in the creation of weapons of mass destruction. The IDAIS dialogues have aimed to build consensus on these red lines, emphasizing clarity and universality as key features for effectiveness, with violations potentially triggering pre-agreed international responses.

**Conditional approaches and the creation of dedicated international bodies represent another key strategy.** "If-Then Commitments" involve developers or states agreeing to enact specific safety measures if AI capabilities reach certain predefined thresholds, allowing preparation without hindering development prematurely, as exemplified by the proposed Conditional AI Safety Treaty. Furthermore, proposals exist for new international institutions, potentially modeled after the International Atomic Energy Agency (IAEA), to monitor AI development, verify compliance with agreements, promote safety research, and potentially centralize or control the most high-risk AI development and distribution.

**Specific governance regimes and supporting structures are also under consideration to enhance international coordination.** Given the global nature of AI, mechanisms like international compute governance aim to monitor and control the supply chains for AI chips and large-scale training infrastructure, although technical feasibility and international cooperation remain challenges. Other proposals include establishing a large-scale international AI safety research body akin to CERN, potentially centralizing high-risk research or setting global standards, and strengthening whistleblower protections through international agreements to encourage reporting of safety concerns within the AI industry.

For more information on these topics, please read the next chapter on AI governance.

<Note title="Is AI Governance useful, desirable and possible?" collapsed={true}>

**Historically, the field of AI safety predominantly focused on technical research, influenced partly by views like Eliezer Yudkowsky's assertion that "Politics is the mind killer."** ([Yudkowsky, 2007](https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer)) For many years, the field thought that engaging with policy and politics was ineffective or even counterproductive compared to directly solving the technical alignment problem, leading many early researchers concerned about AGI to prioritize engineering solutions over governance efforts. Surprisingly, in the beginning, it was even almost discouraged to talk about those risks publicly to avoid the race and avoid bringing in people with "poor epistemic" to the community.

**However, by 2023, ChatGPT was published, got viral, and AI governance gained significant traction as a potentially viable strategy for mitigating AGI risks.** This shift occurred as engagement with policymakers appeared to yield some results, making governance seem more tractable than previously thought ([Akash, 2023](https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk)). Then, influential open letters where published (FLI, CAIS), and shifted the overton window. Consequently, influential organizations like 80,000 Hours adjusted their career recommendations, highlighting AI policy and strategy roles, now above technical alignment research, as top priorities for impact ([80,000 Hours, 2023](https://80000hours.org/career-reviews/ai-policy-and-strategy/)).

**However, the Overton Window for stringent international AI safety measures appears to be shrinking.** While initial statements and efforts by groups like the Future of Life Institute and the Center for AI Safety successfully broadened the public and political discourse on AI risks, subsequent developments, including international summits perceived as weak on safety and shifts in political leadership (such as the election of Donald Trump), have cast doubt on the feasibility of achieving robust international coordination ([Zvi, 2025](https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit)). This has led some within the AI governance field to believe that a significant "warning shot" – a clear demonstration of AI danger – might be necessary to galvanize decisive action, although there is skepticism about whether such a convincing event could actually occur before it's too late ([Segerie, 2024](https://www.lesswrong.com/posts/RYx6cLwzoajqjyB6b/what-convincing-warning-shot-could-help-prevent-extinction)).

**Existing and proposed regulations face significant limitations and potential negative consequences.** For instance, prominent legislative efforts like the EU's AI Act, while groundbreaking in some respects, contain notable gaps ([Miles, 2025](https://milesbrundage.substack.com/p/feedback-on-the-second-draft-of-the)); its Code of Practice has limitations and the Act itself may not adequately cover models deployed outside Europe, purely internal deployments for research, or military applications. A critical concern is the potential for frontier AI labs to engage in secret development races, bypassing oversight – a scenario potentially enabled by policy changes like the revocation of executive orders mandating government reporting on frontier model evaluations ([Kokotajlo, 2025](https://ai-2027.com/)).

Additionally, there are fundamental concerns that governance structures capable of controlling AGI might themselves pose risks, potentially enabling totalitarian control.

**A deeply skeptical perspective suggests that much of the current AI progress narrative and regulatory activity might be performative or "fake."** This "full-cynical model" posits that major AI labs might be exaggerating their progress towards AGI to maintain investor confidence and hype, potentially masking slower actual progress or stagnation in core capabilities ([johnswentworth, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7)). In parallel, it suggests that AI regulation activists and lobbyists might prioritize networking and status within policy circles over crafting genuinely effective regulations, leading to measures focused on easily targeted but potentially superficial metrics (like compute thresholds) rather than addressing fundamental risks ([johnswentworth, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7)). This view implies a dynamic where both labs and activists inadvertently reinforce a narrative of imminent, controllable AI breakthroughs, potentially detached from the underlying reality ([johnswentworth, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/?commentId=aBcAh8H9cSzdXmgb7#aBcAh8H9cSzdXmgb7)).

**However, this cynical "fakeness" perspective is debated.** Critics of the cynical view argue that specific regulatory proposals, like SB 1047, did contain potentially valuable elements (e.g., requiring shutdown capabilities, safeguards, and tracking large training runs), even if their overall impact was debated or ultimately limited ([Charbel-Raphaël, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=J2iPumP29GK9Qrm5p);[ johnswentworth, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=G5zcfntZodH3ZYDaP)). It's acknowledged that regulators operate under real constraints, including the significant influence of Big Tech lobbying, which can prevent the prohibition of technologies without clear evidence of unacceptable risk. Furthermore, the phenomenon of "performative compliance" or "compliance theatre" is recognized, but it is argued that engagement with these imperfect processes is still necessary, and that some legislative steps, like the EU AI Act explicitly mentioning "alignment with human intent," represent potentially meaningful progress ([Katalina Hernandez, 2025](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=NLAW24oxDFuTLT3kx)).

**AI regulation could inadvertently increase existential risk through several pathways** ([1a3orn, 2023](https://www.lesswrong.com/posts/6untaSPpsocmkS7Z3/ways-i-expect-ai-regulation-to-increase-extinction-risk)). Regulations might misdirect safety efforts towards outdated or less relevant compliance issues, diverting attention from more important emerging risks (Misdirected Regulations); bureaucratic processes tend to favor large, established players, potentially hindering smaller, innovative safety research efforts; overly stringent national regulations could drive AI development to less safety-conscious international actors, weakening the initial regulator's influence (Disempowering the Countries Regulating); and regulations, particularly those restricting open-source models or setting high compliance costs, could consolidate power in the hands of the largest capability-pushing companies, potentially stifling alternative safety approaches and accelerating risk (Empowering Dominant Players). But the existence of these arguments is not sufficient for saying that AI regulation is net negative, this is mainly a reminder that we need to be cautious in how to regulate. The devil is in the details.

</Note>

## Risk Management {#04}

**Risk management in AI safety builds on established practices from other industries. **Risk management is not unique to AI safety—it has roots in numerous fields including aerospace, nuclear power, and financial services. Each of these domains has developed sophisticated approaches to identifying, analyzing, and mitigating potential harms. The nuclear industry, for example, employs defense-in-depth strategies with multiple redundant safety systems, while aviation uses rigorous certification processes and ongoing monitoring. Financial risk management focuses on stress testing and capital buffers to prevent systemic collapse. These established frameworks provide valuable precedents for AI safety, though the unique characteristics of AI systems—such as their potential for autonomous decision-making, rapid scaling, and emergent behaviors—require adaptations to traditional risk management approaches.

<Note title="SaferAI’s frontier Risk Management" collapsed={true}>

The contents of this box have been pasted from an article by Simeon Campos ([Campos et al, 2024](https://www.safer-ai.org/research-posts/a-frontier-ai-risk-management-framework-bridging-the-gap-between-current-ai-practices-and-established-risk-management)).

The core goal of this risk management framework workflow is to ensure that risks remain below unacceptable levels at all times through the following process:

1. Define a risk tolerance—a well-characterized acceptable level of risk that should not be exceeded.

2. Through risk modeling, operationalize this risk tolerance into pairs of empirically measurable thresholds:

- Key Risk Indicators (KRIs): measurable signals that serve as proxies for risks (e.g., model performance on specific tasks)

- Key Control Indicators (KCIs): measurable signals that serve as proxies for the effectiveness of mitigations (e.g., success rate of containment measures)

These thresholds follow a three-way relationship: for any given risk tolerance level and KRI threshold, there exists a minimum required KCI threshold that must be met to maintain risk below the tolerance.

1. Implement mitigations to achieve the required KCI thresholds whenever KRI thresholds are reached.

The risk tolerance is distinct from capabilities thresholds and can be defined in two ways:

1. Quantitatively using probability and severity: The preferred approach expresses risk tolerance as a product of quantitative probability and severity per unit of time.

2. Using scenarios with quantitative probabilities: For risks where severity is difficult to quantify, risk tolerance can be expressed as a quantitative probability bound on a qualitatively defined harmful scenario.

The framework takes advantage of the life-cycle of an AI system to minimize the burden on AI developers, once they complete model training:

- To avoid delays during the training phase, all preparatory work that doesn’t require the full model can be done ahead of time: risk modeling, defining risk tolerance, identifying KRI and KCI thresholds, and predicting required mitigations using scaling laws.

- This leaves only KRI measurement and open-ended red-teaming (to identify new risk factors that were not identified in the initial risk modeling) for the training and pre-deployment phase.

On risk governance, the framework describes a corporate structure designed to ensure proportionate accounting of risks in decision-making. It includes:

- Risk owner. The risk owner is a person personally responsible for the management of a particular risk.

- Oversight. The oversight function is board-level oversight of senior management’s decision making regarding risk.

- Audit. The audit function is an independent function isolated from peer pressure dynamics that can challenge decision-making.

<Figure src="./img/h39_Image_25.png" alt="Enter image alt description" number="25" label="3.25" caption="Figure from SaferAI’s frontier risk management: “*The risk management framework introduced in this paper enables its users to implement four key risk management functions: identifying risk (risk identification), defining acceptable risk levels and analyzing identified risks (risk analysis & evaluation), mitigating risk to maintain acceptable levels(risk treatment), and ensuring that organizations have the appropriate corporate structure to execute this workflow consistently and rigorously (risk governance).*” - ([Campos et al, 2024](https://www.safer-ai.org/research-posts/a-frontier-ai-risk-management-framework-bridging-the-gap-between-current-ai-practices-and-established-risk-management))" />

<Figure src="./img/SBK_Image_26.png" alt="Enter image alt description" number="26" label="3.26" caption="[https://ratings.safer-ai.org/](https://ratings.safer-ai.org/), screenshot from SaferAI." />

</Note>

**It is important to also manage the risks that could occur before deployment.** Sporadically, this can also be an error sign or a bug ([Ziegler et al., 2020](https://arxiv.org/abs/1909.08593)). To avoid accidents during training, the training should also be responsible. Model evaluation for extreme risks, which was written by researchers from OpenAI, Anthropic, and DeepMind, lays out a good baseline strategy for what needs to be done before training, during training, before deployment, and after deployment. ([Shevlane et al., 2023](https://arxiv.org/abs/2305.15324))

<Figure src="./img/gAA_Image_27.png" alt="Enter image alt description" number="27" label="3.27" caption="A workflow for training and deploying a model responsibly. ([Shevlane et al., 2023](https://arxiv.org/abs/2305.15324))" />

**Implementing structured risk management processes is essential.** This includes identifying, assessing, mitigating, and monitoring risks throughout the AI lifecycle. Frameworks like the NIST AI RMF provide guidance. The "Three Lines of Defense" model, inspired from other industries, (operational management, risk/compliance functions, internal audit) offers a structure for assigning risk management responsibilities within an organization ([Jonas Schuett, 2022](https://arxiv.org/abs/2212.08364)). The "Affirmative Safety" approach proposes requiring developers to proactively build an evidence-based case demonstrating safety against pre-defined risk thresholds, incorporating both technical and operational evidence ([Akash R. Wasil et al., 2024](https://arxiv.org/abs/2406.15371)).

## Safety Culture {#05}

**AI safety is a socio-technical problem that requires a socio-technical solution.** As such, the resolution to these challenges cannot be purely technical. Safety culture is crucial for numerous reasons. At the most basic level, it ensures that safety measures are at least taken seriously. This is important because a disregard for safety can lead to regulations being bypassed or rendered useless, as is often seen when companies that don't care about safety face audits ([Manheim, 2023](https://www.lesswrong.com/posts/iFLNKgZceYyTdwsGz/safety-culture-for-ai-is-important-but-isn-t-going-to-be)).

**The challenge of industry-wide adoption of technical solutions.** Proposing a technical solution is only the initial step toward addressing safety. A technical solution or set of procedures needs to be internalized by all members of an organization. When safety is viewed as a key objective rather than a constraint, organizations often exhibit leadership commitment to safety, personal accountability for safety, and open communication about potential risks and issues ([Hendrycks et al., 2023](https://arxiv.org/abs/2306.12001)).

**Reaching the standards of the aerospace industry.** In aerospace, stringent regulations govern the development and deployment of technology. For instance, an individual cannot simply build an airplane in their garage and fly passengers without undergoing rigorous audits and obtaining the necessary authorizations. In contrast, the AI industry operates with significantly fewer constraints, adopting an extremely permissive approach to development and deployment, allowing developers to create and deploy almost any model. These models can then be integrated into widely used libraries, such as Hugging Face, and those models can then proliferate with minimal auditing. This disparity underscores the need for a more structured and safety-conscious framework in AI. By adopting such a framework, the AI community can work towards ensuring that AI technologies are developed and deployed responsibly, with a focus on safety and alignment with societal values.

**Safety culture can transform industries.** Norms in the pursuit of safety can be a powerful way to notice and discourage bad actors. In the absence of a strong safety culture, companies and individuals may be tempted to cut corners, potentially leading to catastrophic outcomes ([Manheim, 2023](https://www.lesswrong.com/posts/iFLNKgZceYyTdwsGz/safety-culture-for-ai-is-important-but-isn-t-going-to-be)). The adoption of safety culture in the aerospace sector has transformed the industry, making it more attractive and generating more sales, and long term trust. Similarly, an ambitious AI safety culture would require the establishment of a large AI safety and security industry.

If achieved, safety culture would be a systemic factor that prevents AI risks. Rather than focusing solely on the technical implementation of a particular AI system, attention must also be given to social pressures, regulations, and safety culture. This is why engaging the broader ML community that is not yet familiar with AI Safety is critical ([Hendrycks, 2022](https://www.alignmentforum.org/posts/bffA9WC9nEJhtagQi/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1)).

**How to concretely increase public awareness and safety culture?**

- **Open letters:** Initiatives like open letters, similar to the one from the Future of Life Institute ([Future of Life Institute, 2023](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)), can spark public debate on AI risks.

- **Safety culture promotion:** Advocating for a culture of safety among developers and researchers to preemptively address potential risks, for example by organizing internal training on safety. For example, internal training for cybersecurity is already common for some companies. Opening AI safety courses in universities and training future ML practitioners is also important. Projects like DIP aim to inform the democratic process about risks ([Controlai, 2025](https://controlai.com/dip)).

- **Building consensus:** The Create a global AI risk assessment body, similar to the IPCC for climate change, to standardize and disseminate AI safety findings. The international AI Safety report chaired from Yoshua Bengio is an important milestone in this regard. AN emerging research is also aimed at understanding the roots of expert disagreement ([Severin Field, 2025](https://arxiv.org/abs/2502.14870))., facilitating structured debates or adversarial collaborations ([Guest User, 2023](https://forecastingresearch.org/news/ai-adversarial-collaboration))., and developing shared models of risk ([Martin, 2023](https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk)) can help foster convergence.

- **Scary Demos / Model Organisms:** Creating concrete demonstrations of potential alignment failures (like deception or power-seeking) in controlled environments can help build consensus on the reality and nature of risks and test mitigation strategies([Evan Hubinger et al., 2023](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1)). For example, the demonstration around different types of alignment faking have been pretty effective and were debated a lot in the public discourse ([Greenblatt, 2024](https://arxiv.org/abs/2412.14093)).

- **Public Outreach and Engagement:** Public understanding and opinion significantly influence political will for regulation and the social acceptance of AI technologies ([Baobao Zhang et al., 2019](https://arxiv.org/abs/1912.12835)). Surveys show growing public concern about AI risks (including existential risk) and increasing support for regulation and caution ([Jamie Ballard, 2025](https://today.yougov.com/technology/articles/51803-americans-increasingly-skeptical-about-ai-artificial-intelligence-effects-poll)). Advocacy groups play a role in shaping public discourse, by expanding the Overton Window, and pushing for specific policies like pauses ([Holly Elmore, 2024](https://theinsideview.ai/holly)).

<FootnoteRegistry title="Footnotes" />
