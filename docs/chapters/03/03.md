---
id: 3
title: "AGI Safety Strategies"
sidebar_label: "3.3 AGI Safety Strategies"
sidebar_position: 4
slug: /chapters/03/03
section_description: "If alignment proves elusive, what mechanisms can be implemented to monitor and constrain powerful AI systems, preventing catastrophic outcomes even from misaligned agents?"
reading_time_core: "18 min"
reading_time_optional: "9 min"
# Pagination control - override automatic sidebar-based pagination
pagination_prev: chapters/03/2
pagination_next: chapters/03/4
---
import Video from "@site/src/components/chapters/Video";
import Footnote, { FootnoteRegistry } from "@site/src/components/chapters/Footnote";
import Quote from "@site/src/components/chapters/Quote";
import Note from "@site/src/components/chapters/Note";
import Definition from "@site/src/components/chapters/Definition";

import Figure from "@site/src/components/chapters/Figure";

# AGI Safety Strategies

Unlike misuse, where human intent is the driver of harm, AGI safety primarily concerns the behavior of the AI system itself. The core problems become alignment and control: ensuring that these highly capable, potentially autonomous systems reliably understand and pursue goals consistent with human values and intentions, rather than developing and acting on misaligned objectives that could lead to catastrophic outcomes.

This section explores strategies for AGI safety, which as we explained in the definitions section includes but is not limited to just alignment. We distinguish safety strategies that would apply to human-level AGI from safety strategies which guarantee us safety from ASI. This section focuses on the former, and the next section will focus on ASI.

**AGI safety strategies operate under fundamentally different constraints than ASI approaches.** When dealing with systems at near human-level intelligence, we can theoretically retain meaningful oversight capabilities and can iterate on safety measures through trial and error. Humans can still evaluate outputs, understand reasoning processes, and provide feedback that improves system behavior. This creates strategic opportunities that disappear once AI generality and capability surpass human comprehension across most domains. It is debated if any of the safety strategies intended for human level AGI will continue to work for superintelligence.

**Strategies for AGI and ASI safety often get conflated, stemming from uncertainty about transition timelines.** Timelines are hotly debated in AI research. Some researchers expect rapid capability gains that could compress the period for how long AIs remain human-level into months rather than years ([Soares, 2022](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization); [Yudkowsky, 2022](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities); [Kokotajlo et al., 2025](https://ai-2027.com/)). If the transition from human-level to vastly superhuman intelligence happens quickly, AGI-specific strategies might never have time for deployment. However, if we do have a meaningful period of human-level operation, we have safety options that won't exist at superintelligent levels, making this distinction important for strategic considerations.

## Naïve Strategies {#01}

People discovering the field of alignment often propose many naive solutions. Unfortunately, no simple strategy has withstood criticism. Here are just a few of them.

**Asimov's Laws.** These are a set of fictional rules devised by science fiction author Isaac Asimov to govern the behavior of robots.

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

2. A robot must obey orders given to it by human beings except where such orders would conflict with the First Law.

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

Asimov's Laws of Robotics may seem straightforward and comprehensive at first glance, but in practice they are too simplistic and vague to handle the complexities of real-world scenarios, particularly since harm can be nuanced and context-dependent ([Hendrycks et al., 2022](https://arxiv.org/abs/2306.12001)). For instance, the first law prohibits a robot from causing harm, but what does "harm" mean in this context? Does it only refer to physical harm, or does it include psychological or emotional harm as well? And how should a robot prioritize conflicting instructions that could lead to harm? This lack of clarity can create complications ([Bostrom, 2016](https://doi.org/10.1002/9781118922590.ch23)), implying that having a list of rules or axioms is insufficient to ensure AI systems' safety. Asimov's Laws are inappropriate, and that is why the end of Asimov's story does not turn out well.<Footnote id="footnote_asimov" number="1" text="It's important to note that this critique aligns with Asimov's own intentions rather than contradicting them. The point of every story in “I, Robot” was precisely to lay out plausible scenarios in which the laws break down. Asimov was keenly aware of the limitations of the ethics implied by his laws, and he explored these limitations thoroughly in his robot novels like “The Caves of Steel,“ “The Naked Sun,“ and “The Robots of Dawn.“ In his final robot novel, “Robots and Empire,“ Asimov even introduces the “Zeroth Law“ as a potential approach to dealing with superintelligent AI: preventing robots from taking any action that would allow the destruction of humanity as a whole, even if it means sacrificing the lives of some individuals." /> More generally, designing a good set of rules without holes is very difficult. See the chapter on specification gaming for more details.



**Lack of Embodiment.** Keeping AIs non-physical might limit the types of direct harm they can do. However, disembodied AIs could still cause harm through digital means. For example, even if a competent Large Language Model (LLM) does not have a body, it could hypothetically self-replicate ([Wijk, 2023](https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a)), recruit human allies, tele-operate military equipment, make money via quantitative trading, etc… Also note that more and more humanoid robots are being manufactured so lack of embodiment is also unlikely to apply in practice.

**Raising it like a child.** AI, unlike a human child, lacks structures put in place by evolution crucial for ethical learning and development ([Miles, 2018](https://www.youtube.com/watch?v=eaYIU6YXr3w)). For instance, the neurotypical human brain has mechanisms for acquiring social norms and ethical behavior which are not present in AIs or psychopaths who know right from wrong but don't care ([Cima et al, 2010](https://pmc.ncbi.nlm.nih.gov/articles/PMC2840845/)). These mechanisms were developed over thousands of years of evolution ([Miles, 2018](https://www.youtube.com/watch?v=eaYIU6YXr3w)). We don’t know how to implement this strategy because we don’t know how to create a brain-like AGI ([Byrnes, 2022](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8)). It is also worth noting that human children, despite good education, are also not always guaranteed to act aligned with the overarching interests and values of humanity.

<Video type="youtube" videoId="eaYIU6YXr3w" number="1" label="3.1" caption="Optional video to get an answer to the question - Why can't we just raise the AI like a child?" />

<Note title="The Case for Optimism in Alignment - A Contentious Point" collapsed={true}>

**In 2023, some alignment researchers published "AI is easy to control".** Pope & Belrose (2023) exemplify this stance, estimating AI x-risk at only 1%. One of their justification is that AI is a "white box" whose internals are accessible ([Optimists, 2023](https://optimists.ai/2023/11/28/ai-is-easy-to-control/)). The optimistic view that "AI is easy to control" is rooted in several observations about current AI systems, particularly LLMs. Proponents argue that AI systems offer substantial advantages over humans: they can be reset, we can observe their internal states, we directly control their training data, and we can modify their weights through gradient descent. These features appear to provide powerful levers for ensuring alignment.

**Their essay faced lots of counterarguments.** Critics point out the difficulty of specifying complex human values, the potential for emergent goals, the challenge of out-of-distribution generalization ([Byrnes, 2023](https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose)). Controllability at the technical level doesn't guarantee safety in deployment, as economic and competitive pressures may lead to the creation of systems with increasingly autonomous goals. Second, many current control arguments assume architectural features specific to today's AI systems that may not apply to future designs. For instance, brain-like AGI that performs continuous online learning would invalidate many current safety assumptions.

**A lot of the debate comes from the "White Box" argument.** Optimists often emphasize that AI models are "white boxes" (we can inspect their weights and architecture) unlike "black box" human brains. Byrnes counters that this distinction, while factually true regarding access, can be misleading. He argues the debate over these terms is unproductive. While we can access AI internals unlike brains, understanding why an AI behaves a certain way based on those internals remains profoundly difficult, potentially taking decades of research (unlike debugging traditional software or understanding engineered systems). Therefore, simply stating "AI is a white box" is a naive basis for confidence in control; the interpretability challenge remains immense, this easier access to AI weights is still not sufficient to solve jailbreaks in practice or avoid accidents like[ BingChat threatening users](https://time.com/6256529/bing-openai-chatgpt-danger-alignment/).

</Note>

## Solve Alignment {#02}

### Requirements for AGI Alignment {#02-01}

**Defining even the requirement for an alignment solution remains contentious among researchers.** Before exploring potential paths towards alignment solutions, we need to establish what successful solutions should achieve. The challenge is that we don't really know what they should look like - there's substantial uncertainty and disagreement across the field. However, several requirements do appear relatively consensual ([Christiano, 2017](https://www.alignmentforum.org/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment)):

- **Robustness across distribution shifts and adversarial scenarios.** The alignment solution must work when AGI systems encounter situations outside their training distribution. We can't train AGI systems on every possible situation they might encounter, so safety behaviors learned during training need to generalize reliably to novel deployment scenarios. This includes resistance to adversarial attacks where bad actors deliberately try to manipulate the system into harmful behavior.

- **Scalability alongside increasing capabilities.** As AI systems become more capable, the alignment solution should continue functioning effectively without requiring complete retraining or reengineering. This requirement becomes even more stringent for ASI, where we need alignment solutions that scale beyond human intelligence levels.

- **Technical feasibility within realistic timeframes.** The alignment solution must be achievable with current or foreseeable technology and resources. Solution proposals cannot rely on major unforeseen scientific breakthroughs, or function only as theoretical frameworks with very low Technology Readiness Levels (TRL). <Footnote id="footnote_tech_readiness" number="2" text="The Technology Readiness Levels from NASA is a scale from 1 to 9 to measure the maturity of a technology. Level 1 represents the earliest stage of technology development, characterized by basic principles observed and reported, and level 9 represents actual technology proven through successful mission operations." />

- **Low alignment tax to ensure competitive adoption.** Safety measures cannot impose prohibitive costs in compute, engineering effort, or deployment delays. If alignment techniques require substantially more resources or severely limit capabilities, competitive pressures will push developers toward unsafe alternatives. This constraint exists because multiple actors are racing to develop AGI - if safety measures make one organization significantly slower or less capable, others may skip those measures entirely to gain competitive advantage.



<Figure src="./img/a1J_Image_13.png" alt="Enter image alt description" number="13" label="3.13" caption="Illustration of how applying a safety or alignment technique could make the model less capable. This is called a safety tax." />

**Existing AGI alignment techniques fall dramatically short of these requirements.** Empirical research has demonstrated that AI systems can exhibit deeply concerning behaviors where current alignment research falls short of these requirements. We already have clear demonstrations of models engaging in deception ([Baker et al., 2025](https://arxiv.org/abs/2503.11926); [Hubinger et al., 2024](https://arxiv.org/abs/2401.05566)), faking alignment during training while planning different behavior during deployment ([Greenblatt et al., 2024](https://arxiv.org/abs/2412.14093)), gaming specifications ([Bondarenko et al., 2025](https://arxiv.org/abs/2502.13295)), gaming evaluations to appear more capable than they actually are ([OpenAI, 2024](https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf); [SakanaAI, 2025](https://x.com/SakanaAILabs/status/1892992938013270019)), and in some cases, try to disable oversight mechanisms or exfiltrate their own weights ([Meinke et al., 2024](https://arxiv.org/abs/2412.04984)). Current alignment techniques like RLHF and its variations (Constitutional AI, Direct Preference Optimization, fine-tuning and other RLHF modifications) are fragile and brittle ([Casper et al., 2023](https://arxiv.org/abs/2307.15217')) and without augmentation would not be able to remove the dangerous capabilities like deception. Strategies to solve alignment not only fail to prevent these behaviors but often cannot even detect when they occur ([Hubinger et al., 2024](https://arxiv.org/abs/2401.05566); [Greenblatt et al., 2024](https://arxiv.org/abs/2412.14093)).

**Solving alignment means we need more work on satisfying all these requirements.** The limitations of current techniques point toward specific areas where breakthroughs are needed. All strategies aim to have technical feasibility and low alignment tax, so these are common requirements, however some strategies try to focus on more concrete goals which we will explore through future chapters. Here is a short list of key goals of alignment research:

- **Solving the Misspecification problem:** Being able to specify goals correctly to AIs without unintended side effects. See the chapter on Specification.

- **Solving Scalable Oversight:** After solving the specification problem for human level AI by using techniques like RLHF, and its variations, we need to find methods to ensure AI oversight can detect instances of specification gaming for beyond human level. This includes being able to identify and remove dangerous hidden capabilities in deep learning models, such as the potential for deception or Trojans. See the chapter on Scalable Oversight.

- **Solving Generalization:** Attaining robustness would be key to addressing the problem of goal misgeneralization. See the chapter on Goal Misgeneralization.

- **Solving Interpretability:** Understanding how models operate would greatly aid in assessing their safety, and generalisation properties. Interpretability could, for example, help understand better how models work, and this could be instrumental for other safety goals, like preventing deceptive alignment, which is one type of misgeneralization. See the chapter on Interpretability.

**The overarching strategy requires prioritizing safety research over capabilities advancement.** Given the substantial gaps between current techniques and requirements, the general approach involves significantly increasing funding for alignment research while exercising restraint in capabilities development when safety measures remain insufficient relative to system capabilities.

<Note title="Are misuse and misalignment that different?" collapsed={true}>

AI misuse and rogue AI might be essentially the same scenario in their outcomes, though the only difference is that for misalignment, the initial request to do harm does not come from a human but from an AI. If we build an existentially-risky triggerable system, it's likely to get triggered regardless of whether the initiator is human or artificial ([Shapira, 2025](https://www.youtube.com/watch?v=0KmaotctziE)).

**Nevertheless, these threat models might be strategically pretty different.** AI developers can prevent misuse by not being evil and by preventing people who are evil from using their systems. With rogue AI, it doesn't matter if the developers are good or who gets access - the threat emerges from the system's internal goals or decision-making processes rather than human intent.

**AI-Enabled Coups vs AI takeover represent a critical safety concern.** Tom Davidson and colleagues present a concerning risk scenario ([Davidson, 2025](https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power)): that advanced AI systems could enable a small group of people—potentially even a single person—to seize governmental power through a coup. The authors argue this risk is comparable in importance to AI takeover but much more neglected in current discourse. This threat model closely parallels that of AI takeover, with the key difference being whether power is seized by the AI itself or by humans controlling the AI.

**Common safeguards could protect against both scenarios. **Many of the same mitigations would address both risks, including alignment audits, transparency about capabilities, monitoring AI activities, and strong infosecurity measures that prevent either malicious human control or autonomous harmful behavior.

**Some mitigations target specifically the risk of AI-Enabled Coups.** The report concludes with specific recommendations for AI developers and governments, including establishing rules against AI systems assisting with coups, improving adherence to model specifications, auditing for secret loyalties, implementing strong infosecurity, sharing information about capabilities, distributing access among multiple stakeholders, and increasing oversight of frontier AI projects.

<Figure src="./img/q3d_Image_14.png" alt="Enter image alt description" number="14" label="3.14" caption="According to Richard Ngo, the distinction between misalignment and misuse risks from AI might often be unhelpful. Instead, we should primarily think about ‘misaligned coalitions’ of both humans and AIs, ranging from terrorist groups to authoritarian states. Slide from ([Ngo, 2024](https://www.youtube.com/watch?app=desktop&si=XRR0ofCG7IEp1n_b&v=4v3uqWeVmco&feature=youtu.be))." />

</Note>

## AI Control {#03}

<Figure src="./img/5Oh_Image_15.png" alt="Enter image alt description" number="15" label="3.15" caption="Transformatively useful AI means “Capable of substantially reducing the risk posed by subsequent AIs if fully deployed, likely by speeding up R&D and some other tasks by a large factor (perhaps 30x).” citation from ([Greenblatt & Shlegeris, 2024](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments)) - figure from us;" />

Given the fragility of alignment techniques, one approach to enhancing safety involves controlling AI models by restricting the use of the most dangerous models to monitored environments to prevent them from causing disasters, exfiltrating data, or taking control of the data center ([Shlegeris, 2022](https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root)). This approach often assumes a worst-case scenario where the AI might be actively deceptive or power-seeking.

A central concern in control research is "scheming" – an AI strategically hiding its true (misaligned) goals during training or evaluation, only to pursue them once deployed, potentially seeking power or resisting shutdown ([Greenblatt, 2025](https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming)). Assessing the likelihood of scheming and how we might update on future evidence remains difficult as of 2025 ([Greenblatt, 2025](https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming)). Empirical work suggests current frontier models possess the capability for in-context scheming (reasoning deceptively when prompted with a goal and context), though not necessarily an inherent propensity ([Hobbhahn et al., 2024](https://www.lesswrong.com/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming)). <Footnote id="footnote_control_caveat" number="3" text="Models in this experiment were prompted to scheme." /> We have a lot more information on scheming, and deception in the chapters on evaluations and on goal misgeneralization.



**Ensuring control even without robust alignment.** As we’ve seen in the section on definitions, there is an important distinction between control, which aims to prevent the AI from "leaving the box" (and more generally prevent the AI from causing catastrophes) even if it tries, and alignment, which aims to make the AI want to stay in the box. This strategy aims to keep dangerous AI systems under control even if they are not fully "aligned." Laboratories might need powerful/dangerous AIs for certain use cases (for example, studying the alignment properties of capable AIs or doing research with them, etc.). The AI control agenda targets models with dangerous capabilities and deceptive (or at least superficially aligned) behaviors.

**Control is probably not scalable but doable and complementary to alignment.** Historically, controlling malign superintelligences has been considered impossible, as, for example, a superintelligence could convince the guardians to let it out of the box. Experiments among humans have shown that it is indeed possible for some humans to convince others to let them out after a long discussion ([Yudkowsky, 2002](https://www.yudkowsky.net/singularity/aibox);[ Yudkowsky, 2008](https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible)). We thus assume that controlling superintelligences is extremely complex. However, this does not mean it's impossible to derive intellectual labor from AIs that are not wildly superhuman.

**Evaluating if the AI is deceptive is challenging.** The main problem is that with current techniques, it is impossible to determine if an AI is aligned or if it's simply behaving well while biding its time. It seems difficult to make this distinction for highly competent and situationally aware AIs (see the risk chapter). This could change in the future with techniques enabling us to audit the internals of the model, but for now, those techniques are inaccessible.

**Comparatively, evaluating AI capabilities is easier.** If the AI is not able to do the task even after you have fine-tuned it to do it, you can be confident that the AI is not capable yet of doing the task. Even if the AI is situationally aware and sandbagging its capabilities, it cannot resist the optimization gradient pressure (modulo exploration hacking and other quirks ([Hubinger, 2023](https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations))). Thus, evaluating the upper bound of the capabilities isn't easy, but at least easier than assessing alignment. Specific evaluations, like those done by the Model Evaluation and Threat Research (METR) team, for checking Auto Replication and adaptation abilities ([METR, 2023](https://metr.org/blog/2023-08-01-new-report/)), seem to be reasonable first steps for gauging the power of current AIs. Evaluations that are specifically designed to know if we have control over the AI are called "control evaluations." See the chapter on Evaluation for more information. In summary this involves adversarial testing where a "red team" (representing the potentially misaligned AI) attempts to subvert safety measures implemented by a "blue team" (representing the developers/deployers) within a controlled proxy environment.

The most straightforward strategies for establishing control is to design a security protocol that contains the following elements, for example:

1. **Security and Containment:** This method (sometimes referred to as "boxing") confines an AI within a restricted environment to prevent its broader influence, effectively creating a secure enclosure for the AI. However, human-level AIs could potentially deceive human guardians or exploit vulnerabilities to escape their confinement. Without further operationalization, this approach remains a naive strategy. Nonetheless, it is possible to operationalize this further and evaluate more rigorously the effectiveness of the containment system: Evaluating if the AI can escape its bounded environment when we train it to do so is a capability evaluation, enabling us to use capability evaluations to assess security measures. The security protocol needs to be designed, detailed and tested with specific threat models in mind.

2. **Monitoring:** This strategy involves continuous oversight of AI behavior through automated systems, human supervision, or a combination of both to detect and respond to undesirable actions in real-time. Effective monitoring requires setting up detailed criteria for what constitutes "undesirable behavior" and employing sophisticated detection algorithms that can identify subtle signs of deviation from expected behavior patterns. This could include monitoring for attempts at deception, unauthorized access to data, or unexpected changes in the AI's operation. The challenge with monitoring lies in balancing the sensitivity of detection mechanisms to catch genuine threats without overwhelming the system with false alarms. Additionally, monitoring systems themselves must be designed to resist manipulation by the AI to ensure that they cannot be disabled, tricked, or bypassed ([Greenblatt et al., 2024](https://arxiv.org/abs/2312.06942)).

There are still many problems to address, but in summary, it seems possible to verify the robustness of a security protocol through control evaluations by taking precautions against different failure modes. For an overview of the control agenda, [this sequence](https://www.alignmentforum.org/s/WCJtsn6fNib6L7ZBB) is a good starting point.

<Note title="Does the AI Control line of research meaningfully reduce AI risks?" collapsed={true}>

Even the control agenda suffers from many critiques.

Jan Kulveit argues that controlling AI without aligning them might reduce the probability of a medium-sized warning shot that does not kill everyone ([Kulveit, 2025](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)). "I. In worlds where we are doing reasonably well on alignment, AI control agenda does not have much impact. II. In worlds where we are failing at alignment, AI control may primarily shift probability mass away from 'moderately large warning shots' and towards 'ineffective warning shots' and 'existential catastrophe, full takeover'."

The article, "The Case Against AI Control Research" ([Wentworth, 2025](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research)), argues that AI control research, as commonly defined and practiced, addresses a relatively small portion of the existential risk (X-risk) posed by artificial intelligence. Wentworth argues we won't be able to recognize a good solution to alignment. The author posits that the most likely failure scenario involves early transformative AI producing flawed, misleading, or subtly incorrect solutions ("slop") when tasked with solving superintelligence alignment. Labs might mistakenly accept these flawed solutions due to the difficulty of verification, AI sycophancy, researcher error, or organizational problems, and then proceed to build misaligned superintelligence based on them. Because control research narrowly focuses on intentional scheming by early AI, it doesn't address the larger risk posed by "slop" and the challenge of ensuring early AI genuinely contributes to solving the alignment problem for more advanced systems rather than just being a superhuman sycophant that convinces humans that their flawed idea of how to align an AI is correct when it's not.

Both arguments are debatable, but the general point is that it is hard to guarantee that even promising-looking lines of research are not, in fact, net negative.

</Note>

<Note title="How to box an AI" collapsed={true}>

While boxing may not be scalable, it remains a neglected strategy.

Many people think that trying to box powerful AIs is futile. This is often fueled by instances where humans, playing the AIs in the Box Experiment, have managed to convince others to 'release' them ([Yudkowsky, 2002](http://yudkowsky.net/singularity/aibox/);[ Corwin, 2002](http://sl4.org/archive/0207/4935.html)). Those historical examples must be taken seriously.

AIs could use tactics such as promising vast wealth, power, and intelligence to those who free them, asserting that they alone can avert existential threats or other appeals to emotion.

Nonetheless, we maintain that a variety of strategies can substantially reduce the risk of AI escape, even if we acknowledge there exists a level of intelligence that might inevitably persuade any group of humans to aid in its liberation. Containment may be more achievable than alignment, particularly at the human-level intelligence stage. It may be possible to conceive protocols that significantly increase the difficulty for AI to go out of the box.

Boxing can be conceptualized as a game requiring preparation. The general strategy is to prepare and not let the AI prepare.

Possible mitigation strategies include:

- No single individual should have the capacity to release the AI.

- Engaging in counter-strategy practices: Understand some common breakout strategies (accessible through resources like [this one](https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost)).

- Testing and training humans, utilizing resistance psychology evaluations to select suitable individuals for containment tasks, such as for space training.

- Monitoring AI persuasiveness, as outlined in the preparedness framework. ([OpenAI, 2023](https://cdn.openai.com/openai-preparedness-framework-beta.pdf))

Many more concrete strategies are listed [here](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled#comments), and a summary of discussions on this topic is available [here](https://www.lesswrong.com/tag/ai-boxing-containment).

</Note>

## Transparent Thoughts {#04}

<section-header>

Can we design or incentivize AI systems to externalize their reasoning processes in an understandable format (like natural language), thereby enabling more effective monitoring?

</section-header>

Monitoring the Chain-of-Thought (CoT) – the explicit natural language reasoning steps produced by some models – has been explored as a specific control and interpretability technique. Transparency is instrumental in both alignment and control.

The hope is that CoT provides a window into the AI's "thinking," allowing monitors (human or AI) to detect malicious intent or flawed reasoning before harmful actions occur ([Openai, 2025](https://openai.com/index/chain-of-thought-monitoring/)). OpenAI found CoT monitoring effective for detecting reward hacking in programming tasks, especially when the monitor could see the CoT alongside the actions.

<Figure src="./img/fTP_Image_16.png" alt="Enter image alt description" number="16" label="3.16" caption="Figure from The Translucent Thoughts Hypotheses and Their Implications ([Roger, 2023](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications)). If the AI we are working with can only reason through chains of thought in English, then it becomes significantly easier to monitor its thoughts." />

Unfortunately, the vast majority of AI do not reason in English; for example, AlphaGo reasons in a language that is entirely different from English. LLMs, on the other hand, are capable of reasoning in English, but the apparent reasoning that is written does not represent their underlying thought process ([Turpin et al., 2023](https://arxiv.org/abs/2305.04388)).

**It is probably much easier to align and monitor an AI that only thinks in English** and that is pre-trained on human-generated text, than to align AGIs trained from scratch with RL/evolutionary methods because those latter architectures would create their own language and would be totally alien. Assuming that an LLM cannot devise a plan to destroy the world during a single forward pass, it is plausible that such AIs would be monitorable because we would be able to look into the text generated to see if anything is suspicious. The set of hypotheses required to assert that the first AGI is going to be monitorable if we just look at its reasoning in English are described in the post: The Translucent Thoughts Hypotheses and Their Implications ([Roger, 2023](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications)).

**It might be possible to incentivize LLMs to think more plainly in English.** Obviously, the first step is to train them to think in English. But even this is insufficient. Training LLMs to reason by skipping steps incentivizes them to reason in a hidden and non-transparent manner, and this is what happens implicitly when we train LLMs to predict the next token. Conversely, if we train LLMs to detail all their reasoning steps, we can reduce incentives for hidden reasoning. This approach is also known as process-based training, which involves training AI systems to reason only step-by-step, and we detail this in the scalable oversight chapter. Unfortunately, the above reasoning is not straightforward; AIs may learn to utilize hidden reasoning even if they are trained to detail all reasoning steps during training, such as steganography, or with architecture such as COCONUT (see below).<Footnote id="footnote_steganography" number="4" text="Steganography is a hidden coded language. For example, the first letter of a sentence gives the next letter of the message." />



**Some AI architectures prevent us from using this strategy altogether.** Meta's paper Training Large Language Models to Reason in a Continuous Latent Space ([Hao, et ao, 2024](https://arxiv.org/abs/2412.06769)) presents an architecture that prevents us from reading the chain of thought from the AI systems. If we want to be able to monitor what's going on, it may be important not to use these architectures, even if they do enable capability gains.

<Figure src="./img/Lo9_Image_17.png" alt="Enter image alt description" number="17" label="3.17" caption="Figure 1 from the COCONUT papers. ([Hao et al., 2024](https://arxiv.org/abs/2412.06769))" />

Techniques like paraphrasing can help prevent steganography, but this requires a lot of subtleties ([Roger & Greenblatt, 2023](https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning)).

Here are a few operationalizations of the transparent thoughts agenda:

- One implementation of the transparent thought agenda is to use cognitive emulation, which is the current technical plan of Conjecture ([Leahy & Alfour, 2023](https://www.alignmentforum.org/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal)). Cognitive Emulation (CoEm) would involve emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in plain English to each other before outputting the next token. If the neural network used deceptive thoughts, we could observe them in these intermediate messages. Some caveats and technical details on CoEm are available[ here](https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design).

- Anthropic’s externalized reasoning oversight ([Lanham, 2022](https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for)) is measured by the faithfulness of the chain-of-thought reasoning ([Lanham et al., 2023](https://arxiv.org/abs/2307.13702)) or by decomposing questions and subquestions to improve the faithfulness of the answer ([Radhakrishnan et al., 2023](https://arxiv.org/abs/2307.11768)).

- Improving mathematical reasoning with process supervision. Instead of creating an AI that directly gives the final answer, you train it to output the reasoning step by step. This improves the transparency and also the performance! ([Lightman et al, 2023](https://arxiv.org/abs/2305.20050))

In summary, a general strategy to create a safe by-design AI is to ensure that AIs externalize their reasoning, making monitoring this chain of thought easier. This could potentially be simpler than probing into the models' internals with interpretability. Therefore, it is crucial to avoid building AIs that are trained and incentivized to internalize much of their thoughts.

**We now have extensive evidence that chain of thought monitoring is not entirely faithfully.** Anthropic's research further demonstrated that models often fail to faithfully report their reasoning in CoT, omitting influential factors (like hints) or fabricating justifications, particularly when incentivized to reward hack ([Anthropic, 2025](https://www.anthropic.com/research/reasoning-models-dont-say-think)). This suggests CoT may not be a reliable indicator of a model's true reasoning process, especially under adversarial pressure. The debate continues on whether CoT monitoring can be made robust enough for high-stakes safety.

**However, some researchers argue these findings do not entirely invalidate the potential of CoT monitoring for detecting sophisticated misalignment.** As Greenblatt (2025) [noted](https://x.com/RyanPGreenblatt/status/1908298064366100676), the unfaithfulness observed in those studies primarily occurs in scenarios where reasoning is minimal enough that extensive CoT isn't necessary. The more concerning alignment risks—such as alignment faking, training gaming, or sophisticated deception—would likely require complex reasoning processes that might still leave traces in CoT. Non-trivial harmful outcomes such as escape attempts or research sabotage would similarly demand sophisticated reasoning that could potentially be detected.

Nevertheless, several factors might ultimately limit CoT monitoring effectiveness in advanced systems. These include architectural changes enabling opaque recurrent processing (sometimes called "neuralese"), the development of steganographic capabilities that could hide reasoning within seemingly benign outputs, or simply much stronger forward passes that require less explicit reasoning. Of these, the development of opaque reasoning abilities through neural recurrence may represent the most significant challenge to maintaining the transparency needed for effective monitoring. This whole agenda depends notably on the hypothesis that "Effective plans to defeat humanity cannot be found in a single LLM forward pass." Once AIs become too strong in a forward pass, most hopes of translucent thoughts go away, so either you need to succeed at ambitious CoEm by building a giant superhuman transparent bureaucracy, or we need new approaches. This is the topic of ASI Safety.

<Note title="The Most Forbidden Technique" collapsed={true}>

The Most Forbidden Technique is training an AI using interpretability techniques. This term has been coined by Zvi Mowshowitz ([Mowshowitz, 2025](https://thezvi.substack.com/i/145384938/the-art-of-the-jailbreak)). OpenAI's research showed that directly optimizing the CoT for safety (i.e., penalizing "bad thoughts") caused models to learn to hide their deceptive reasoning while still misbehaving ([OpenAI, 2025](https://openai.com/index/chain-of-thought-monitoring/)).

<Quote speaker="Zvi Mowshowitz" position="" date="" source="">

An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.

You train on [X]. Only [X]. Never [M], never [T].

Why? Because [T] is how you figure out when the model is misbehaving.

If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.

Those bits of optimization pressure from [T] are precious. Use them wisely.

</Quote>

</Note>

<FootnoteRegistry title="Footnotes" />
