---
id: 1
title: "Definitions"
sidebar_label: "3.1 Definitions"
sidebar_position: 2
slug: /chapters/03/01
reading_time_core: "8 min"
reading_time_optional: "7 min"
---
import Footnote, { FootnoteRegistry } from "@site/src/components/chapters/Footnote";
import Note from "@site/src/components/chapters/Note";
import Quote from "@site/src/components/chapters/Quote";
import Definition from "@site/src/components/chapters/Definition";

# Definitions

**Definitions shape strategy selection.** How we define problems directly impacts which strategies we pursue in solving that problem. In a new and evolving field like AI safety, clearly defined terms are essential for effective communication and research. Ambiguity leads to miscommunication, hinders collaboration, obscures disagreements, and facilitates safety washing ([Ren et al., 2024](https://arxiv.org/abs/2407.21792); [Lizka, 2023](https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing)). The terms we use reflect our assumptions about the nature of the problems we're trying to solve and shape the solutions we develop. Terms like "alignment" and "safety" are used with varying meanings, reflecting different underlying assumptions about the nature of the problem and the goals of the research. The goal of this section is to explain different perspectives around these words, what exactly specific safety strategies are aiming at, and establish how our text will use them.

## AI Safety {#01}

**AI safety ensures AI systems do not cause harm to humans or the environment.** It encompasses the broadest range of research and engineering practices focused on preventing harmful outcomes from AI systems. While alignment focuses on things like an AI's goals and intentions, safety addresses a wider spectrum of concerns ([Rudner et al., 2021](https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/)). It is concerned with ensuring that AI systems do not inadvertently or deliberately cause harm or danger to humans or the environment. AI safety research aims to identify causes of unintended AI behavior and develop tools for safe and reliable operation. It can include technical subfields like robustness (ensuring reliable performance, including against adversarial attacks), monitoring (observing AI behavior), and capability control (limiting potentially dangerous abilities).

<Definition term="AI Safety" source="">

Ensuring that AI systems do not inadvertently or deliberately cause harm or danger to humans or the environment, through research that identifies causes of unintended AI behavior and develops tools for safe and reliable operation.

</Definition>

## AI Alignment {#02}

**AI alignment aims to ensure AI systems act in accordance with human intentions and values.** Alignment is a subset of safety focusing specifically on ensuring AI objectives match human intentions and values. Theoretically, a system could be aligned but unsafe (e.g., competently pursuing the wrong goal due to misspecification) or safe but unaligned (e.g., constrained by control mechanisms despite misaligned objectives). While this sounds straightforward, the precise scope varies significantly across research communities. We already saw a short definition of alignment in the previous chapter, but this section goes deeper and provides a much more nuanced perspective on the different definitions that we could potentially work with.

**Broader definitions of alignment encompass the entire challenge of creating beneficial AI outcomes.** These approaches focus on ensuring AI systems understand and properly implement human preferences ([Christiano, 2018](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)), addressing complex value learning challenges ([Dewey, 2011](https://intelligence.org/files/LearningValue.pdf)), and incorporating robustness aspects like resistance to jailbreaking ([Jonker et al., 2024](https://www.ibm.com/think/topics/ai-alignment)). This comprehensive view treats alignment as including both the intent of the system and its competence in understanding human values - essentially addressing the full spectrum of what makes an AI system behave in ways humans would approve of<Footnote id="alignment_hedging" number="1" text="While AI alignment does not necessarily encompass all systemic risks and misuses, there is some overlap. Some alignment techniques could help mitigate certain misuse scenarios—for instance, alignment methods could ensure that models refuse to cooperate with users intending to use AI for harmful purposes like bioterrorism. Similarly, from a systemic risk perspective, a well-aligned AI might recognize and refuse to participate in problematic processes embedded in systems like financial markets. However, challenges remain, as malicious actors might attempt to circumvent these protections through targeted fine-tuning of models for harmful purposes, and in this case even a perfectly aligned model wouldn't be able to resist." />.

**Narrower definitions of alignment focus specifically on the AI's motivation and intent independent of outcomes.** Some definitions are much more narrow and focus specifically on the AI's motivation - "*An AI (A) is trying to do what a human operator (H) wants it to do*" ([Christiano, 2018](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)). This emphasizes the AI's motivation rather than its competence or knowledge. Under this definition, an intent-aligned AI might still fail due to misunderstanding the operator's wishes or lacking knowledge about the world, but it is fundamentally trying to be helpful. Proponents argue this narrow focus isolates the core technical challenge of getting AI systems to adopt human goals, separate from broader issues like value clarification or capability robustness. That is, as long as the agent "means well", it is aligned, even if errors in its assumptions about the user's preferences or about the world at large lead it to actions that are bad for the user.

**The choice of definition reflects underlying assumptions about AI risk and promising solutions.** Focusing narrowly on intent alignment prioritizes research on inner/outer alignment problems, whereas broader views incorporate value learning or robustness research more centrally. These different approaches lead to different research priorities and safety strategies.



**Applying concepts like "trying," "wanting," or "intent" to AI systems is non-trivial.** When we train AI systems, we specify an optimization objective (like maximizing a reward function), but this doesn't necessarily translate to the system "intending" to pursue that objective in a human-like way. Like we explained in the previous chapter, specification failures occur when what we specify doesn't capture what we actually want (well intended pursuit of a bad goal). But solving this is insufficient, it could also pursue completely different goals altogether. As an analogy, think about how evolution "optimized" humans for genetic fitness (optimization objective), yet humans developed other goals (like art appreciation or contraception) that don't maximize reproductive fitness. Similarly, AI systems optimized for certain objectives might develop internal "goals" that don't directly match those objectives, especially as they become more capable.

**"Aligned to whom?" remains a fundamental question with no consensus answer.** Should AI systems align to the immediate operator ([Christiano, 2018](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)), the system designer ([Gil, 2023](https://forum.effectivealtruism.org/posts/6aYfWyo9DKEheogf8/don-t-call-it-ai-alignment)), a specific group of humans, humanity as a whole ([Miller, 2022](https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans)), objective ethical principles, or the operator's hypothetical informed preferences? There are no agreed upon answers to any of these questions, just many different perspectives each with their own set of pros and cons. We have tried to summarize some of the positions in the optional reading boxes below.

<Note title="What do we align the AI to? - Coherent Aggregated Volition (CAV) vs Coherent Extrapolated Volition (CEV)" collapsed={true}>

**Coherent Extrapolated Volition (CEV) attempts to identify what humans would collectively want if we were smarter, more informed, and more morally developed.** It proposes that instead of directly programming specific values into a superintelligent AI, we should program it to figure out what humans would want if we overcame our cognitive limitations. When we train AI systems on current human preferences, we risk encoding our biases, contradictions, and shortsightedness. CEV instead asks: what "would we want" if we knew more, thought faster, or were more the people we wished to be, and had grown up further together? Essentially picture the ideal version of humanity that could theoretically exist in the future. Tell the AI to take actions according to that ([Yudkowsky, 2004](https://intelligence.org/files/CEV.pdf)).

CEV tried to create a path for AI to respect our deeper intentions rather than our immediate desires. The practical implementation of CEV remains speculative. It would require sophisticated modeling of human psychology, ethical development, and social dynamics—capabilities beyond current AI systems. Modern approaches like RLHF (Reinforcement Learning from Human Feedback) can be seen as primitive precursors that align AI with current human preferences rather than extrapolated ones. Constitutional AI frameworks move slightly closer to CEV by trying to encode higher-level principles rather than specific preferences, but still fall far short of full extrapolation.

**Coherent Aggregated Volition (CAV) finds a coherent set of goals and beliefs that best represents humanity's current values without attempting to extrapolate future development.** Ben Goertzel proposed this alternative to CEV, focusing on current human values rather than speculating about our idealized future selves. CAV treats goals and beliefs together as "gobs" (goal and belief sets) and seeks to find a maximally consistent, compact set that maintains similarity to diverse human perspectives. Unlike CEV, which assumes our values would converge if we became more enlightened, CAV acknowledges that fundamental value differences might persist. It aims to create a coherent aggregation that balances different perspectives rather than trying to predict how those perspectives might evolve. This makes CAV potentially more feasible to implement, as it works with observable current values rather than hypothetical future ones ([Goertzel, 2010](https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html)).

**Coherent Blended Volition (CBV) emphasizes that human values should be creatively "blended" through human-guided processes rather than algorithmically averaged or extrapolated.** CBV refines CAV by addressing potential misinterpretations. When discussing value aggregation, many assume it means simple averaging or majority voting. CBV instead proposes a creative blending process that produces new, harmonious value systems that all participants would recognize as adequately representing their contributions. The concept draws from cognitive science theories of conceptual blending, where new ideas emerge from the creative combination of existing ones. In this framework, the process of determining AI values would be guided by humans through collaborative processes rather than delegated to AI systems. This addresses concerns about AI paternalism, where machines might override human autonomy in the name of our "extrapolated" interests ([Goertzel & Pitt, 2012](https://jetpress.org/v22/goertzel-pitt.htm)).

CBV connects to contemporary discussions about participatory AI governance and democratic oversight of AI development. Systems like vTaiwan have implemented CBV-like processes for technology policy development ([vTaiwan, 2023](https://info.vtaiwan.tw/)), showing how human-guided blending can work in practice.

</Note>

<Note title="Who do we align the AI to? - Single-Single or Multi-Multi" collapsed={true}>

**Single-Single Alignment: Getting a single AI system to reliably pursue the goals of a single human operator.** We haven't even solved this, and it presents significant challenges. An AI could be aligned to follow literal commands (like "fetch coffee"), interpret intended meaning (understanding that "fetch coffee" means making it the way you prefer it), pursue what you should have wanted (like suggesting tea if coffee would be unhealthy), or act in your best interests regardless of commands. Following literal commands often leads to failures of specification that we talk about later in the section. Most often researchers use the word alignment to mean the "intent alignment" ([Christiano, 2018](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)), and some more philosophical discussions go into the third - do what I (or humanity) would have wanted. This involves things like coherent extrapolated volition (CEV) ([Yudkowsky, 2004](https://intelligence.org/files/CEV.pdf)), coherent aggregated volition (CAV) ([Goertzel, 2010](https://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html)), and various other lines of thought that go into meta-ethics discourse. We will not be talking extensively about philosophical discourse in this text, and will stick largely to intent alignment and a machine learning perspective. When we use the word "alignment" in this text, we will basically be referring to problems and failures from single-single alignment. Other types of alignment have been historically very under researched, because people have mostly been working with the idea of a singular superintelligence which interacts with humanity as a singular monolith.

**Single-Multi Alignment - Aligning Many AIs to One Human.** If we think ASI will be composed of smaller intelligences which are working together, delegating tasks, and functioning together as a superorganism, then all of the problems of single single alignment would still remain because we still need to figure out single-single before we attempt single-multi. Ideally we don't want any single human (or a very small group of humans) to be in charge of a superintelligence (assuming benevolent dictators don't exist).

**Multi-Single alignment - aligning one AI to many humans.** When multiple humans share control of a single AI system, we face the challenge of whose values and preferences should take priority. Rather than trying to literally aggregate everyone's individual preferences (which could lead to contradictions or lowest-common-denominator outcomes), a more promising approach is aligning the AI to higher-level principles and institutional values - similar to how democratic institutions operate according to principles like transparency and accountability rather than trying to directly optimize for every citizen's preferences.

**Multi-Multi Alignment - aligning many AIs to many humans to many AIs.** This is the most complicated scenario involving multiple AI systems interacting with multiple humans. Here, the distinction between misalignment risk (AIs gaining illegitimate power over humans) and misuse risk (humans using AIs to gain illegitimate power over others) begins to blur. The key challenge becomes preventing problematic concentrations of power while enabling beneficial cooperation between humans and AIs. This requires careful system design that promotes aligned behavior not just at the individual level but across the entire network of human-AI interactions.

**It is unclear if solving single-single alignment would be enough.** Even if we could ensure that every AI system is perfectly aligned with its respective human principal's intentions, we would still face serious risks when these systems interact. This is because different principals may have conflicting interests, or because the systems may fail to coordinate effectively even when their goals align. Perfect individual alignment cannot guarantee safe collective behavior, just as aligning every driver with traffic laws doesn't prevent traffic jams or accidents ([Hammond et al., 2025](https://arxiv.org/abs/2502.14143)). Essentially, if we have three subproblems of alignment within a single agent, then we have three more sub-problems of - miscoordination, conflict, and collusion, when these individual agents start interacting with each other. Each represents a different way multi-agent systems can fail, even if the individual agents appear to function correctly in isolation. There are yet more ways even beyond this when we start to consider emergent effects of interactions between complex systems and gradual disempowerment like we talked about in the chapter on risks.

</Note>

## AI Ethics {#03}

**AI ethics is the field that examines the moral principles and societal implications of AI systems.** It addresses the ethical considerations of potential societal upheavals resulting from AI advancements, and the moral frameworks necessary to navigate these changes. The core of AI ethics lies in ensuring that AI developments are aligned with human dignity, fairness, and societal well-being, through a deep understanding of their broader societal impact. Research in AI ethics would encompass for example privacy norms, identifying and mitigating bias in systems ([Huang et al., 2022](https://ieeexplore.ieee.org/abstract/document/9844014); [Harvard, 2025](https://careerservices.fas.harvard.edu/blog/2025/05/01/what-is-ai-ethics/); [Khan et al., 2022](https://arxiv.org/abs/2109.07906)).

<Definition term="AI Ethics" source="([Huang et al., 2023](https://ieeexplore.ieee.org/abstract/document/9844014))">

The study and application of moral principles to AI development and deployment, addressing questions of fairness, transparency, accountability, privacy, autonomy, and other human values that AI systems should respect or promote.

</Definition>

Ethics complements technical safety approaches by providing normative guidance on what constitutes beneficial AI. While alignment focuses on ensuring AI systems pursue intended objectives, focusing on values or ethics addresses which objectives are worth pursuing ([Huang et al., 2023](https://ieeexplore.ieee.org/abstract/document/9844014); [LaCroix & Luccioni, 2022](https://arxiv.org/abs/2204.05151)). AI ethics might also include discussions of digital rights and potentially even the rights of AIs themselves in the future.

This chapter focuses primarily on safety frameworks as they inform technical safety and governance strategies rather than exploring ethical questions or digital rights considerations that are beyond the scope of this text.

## AI Control {#04}

**AI control ensures systems remain under human authority despite potential misalignment.** AI control implements mechanisms to ensure AI systems remain under human direction, even when they might act against our interests. Unlike alignment approaches that focus on giving AI systems the right goals, control addresses what happens if those goals diverge from human intentions ([Greenblatt et al., 2024](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled)).

<Definition term="AI Control" source="([Greenblatt et al., 2024](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled))">

The technical and procedural measures designed to prevent AI systems from causing unacceptable outcomes, even if these systems actively attempt to subvert safety measures. Control focuses on maintaining human oversight regardless of whether the AI's objectives align with human intentions.

</Definition>

**Control and alignment work as complementary safety approaches.** While alignment aims to prevent preference divergence by designing systems with the right objectives, control creates layers of security that function even when alignment fails. Control measures include monitoring AI actions, restricting system capabilities, human auditing processes, and mechanisms to terminate AI systems when necessary ([Greenblatt et al., 2023](https://arxiv.org/abs/2312.06942)). Some researchers argue that even if alignment is necessary for superintelligence-level AIs, control through monitoring may be a working strategy for less capable systems ([Greenblatt et al., 2024](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled)). Ideally, an AGI would be aligned and controllable, meaning it would have the right goals and be subject to human oversight and intervention if something goes wrong.

We just present a very short overview here. The control line of AI safety research is discussed in much more detail in our chapter on AI evaluations.

<FootnoteRegistry title="Footnotes" />
